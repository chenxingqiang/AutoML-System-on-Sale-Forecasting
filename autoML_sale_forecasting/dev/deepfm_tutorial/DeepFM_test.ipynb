{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "##################################\n",
    "# 0. Functions\n",
    "##################################\n",
    "class Config(object):\n",
    "    \"\"\"\n",
    "    用来存储一些配置信息\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_dict = None\n",
    "        self.feature_size = None\n",
    "        self.field_size = None\n",
    "        self.embedding_size = 8\n",
    "\n",
    "        self.epochs = 20\n",
    "        self.deep_layers_activation = tf.nn.relu\n",
    "\n",
    "        self.loss = \"mse\"\n",
    "        self.l2_reg = 0.1\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "def FeatureDictionary(dfTrain=None, dfTest=None, numeric_cols=None, ignore_cols=None):\n",
    "    \"\"\"\n",
    "    目的是给每一个特征维度都进行编号。\n",
    "    1. 对于离散特征，one-hot之后每一列都是一个新的特征维度。所以，原来的一维度对应的是很多维度，编号也是不同的。\n",
    "    2. 对于连续特征，原来的一维特征依旧是一维特征。\n",
    "    返回一个feat_dict，用于根据 原特征名称和特征取值 快速查询出 对应的特征编号。\n",
    "    :param dfTrain: 原始训练集\n",
    "    :param dfTest:  原始测试集\n",
    "    :param numeric_cols: 所有数值型特征\n",
    "    :param ignore_cols:  所有忽略的特征. 除了数值型和忽略的，剩下的全部认为是离散型\n",
    "    :return: feat_dict, feat_size\n",
    "             1. feat_size: one-hot之后总的特征维度。\n",
    "             2. feat_dict是一个{}， key是特征string的col_name, value可能是编号（int），可能也是一个字典。\n",
    "             如果原特征是连续特征： value就是int，表示对应的特征编号；\n",
    "             如果原特征是离散特征：value就是dict，里面是根据离散特征的 实际取值 查询 该维度的特征编号。 因为离散特征one-hot之后，一个取值就是一个维度，\n",
    "             而一个维度就对应一个编号。\n",
    "    \"\"\"\n",
    "    assert not (dfTrain is None), \"train dataset is not set\"\n",
    "    assert not (dfTest is None), \"test dataset is not set\"\n",
    "\n",
    "    # 编号肯定是要train test一起编号的\n",
    "    df = pd.concat([dfTrain, dfTest], axis=0)\n",
    "\n",
    "    # 返回值\n",
    "    feat_dict = {}\n",
    "\n",
    "    # 目前为止的下一个编号\n",
    "    total_cnt = 0\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in ignore_cols: # 忽略的特征不参与编号\n",
    "            continue\n",
    "\n",
    "        # 连续特征只有一个编号\n",
    "        if col in numeric_cols:\n",
    "            feat_dict[col] = total_cnt\n",
    "            total_cnt += 1\n",
    "            continue\n",
    "\n",
    "        # 离散特征，有多少个取值就有多少个编号\n",
    "        unique_vals = df[col].unique()\n",
    "        unique_cnt = df[col].nunique()\n",
    "        feat_dict[col] = dict(zip(unique_vals, range(total_cnt, total_cnt + unique_cnt)))\n",
    "        total_cnt += unique_cnt\n",
    "\n",
    "    feat_size = total_cnt\n",
    "    return feat_dict, feat_size\n",
    "\n",
    "def parse(feat_dict=None, df=None, has_label=False):\n",
    "    \"\"\"\n",
    "    构造FeatureDict，用于后面Embedding\n",
    "    :param feat_dict: FeatureDictionary生成的。用于根据col和value查询出特征编号的字典\n",
    "    :param df: 数据输入。可以是train也可以是test,不用拼接\n",
    "    :param has_label:  数据中是否包含label\n",
    "    :return:  Xi, Xv, y\n",
    "    \"\"\"\n",
    "    assert not (df is None), \"df is not set\"\n",
    "\n",
    "    dfi = df.copy()\n",
    "\n",
    "    if has_label:\n",
    "        y = df['sale_qty'].values.tolist()\n",
    "        dfi.drop(['date','storeid','goodsid','sale_qty'],axis=1, inplace=True)\n",
    "    else:\n",
    "        ids = dfi[['date','storeid','goodsid']].values.tolist() # 预测样本的ids\n",
    "        dfi.drop(['date','storeid','goodsid'],axis=1, inplace=True)\n",
    "\n",
    "    # dfi是Feature index,大小和dfTrain相同，但是里面的值都是特征对应的编号。\n",
    "    # dfv是Feature value, 可以是binary(0或1), 也可以是实值float，比如3.14\n",
    "    dfv = dfi.copy()\n",
    "\n",
    "    for col in dfi.columns:\n",
    "        if col in IGNORE_FEATURES: # 用到的全局变量： IGNORE_FEATURES, NUMERIC_FEATURES\n",
    "            dfi.drop([col], axis=1, inplace=True)\n",
    "            dfv.drop([col], axis=1, inplace=True)\n",
    "            continue\n",
    "\n",
    "        if col in NUMERIC_FEATURES: # 连续特征1个维度，对应1个编号，这个编号是一个定值\n",
    "            dfi[col] = feat_dict[col]\n",
    "        else:\n",
    "            # 离散特征。不同取值对应不同的特征维度，编号也是不同的。\n",
    "            dfi[col] = dfi[col].map(feat_dict[col])\n",
    "            dfv[col] = 1.0\n",
    "\n",
    "    # 取出里面的值\n",
    "    Xi = dfi.values.tolist()\n",
    "    Xv = dfv.values.tolist()\n",
    "\n",
    "    del dfi, dfv\n",
    "    gc.collect()\n",
    "\n",
    "    if has_label:\n",
    "        return Xi, Xv, y\n",
    "    else:\n",
    "        return Xi, Xv, ids\n",
    "\n",
    "##################################\n",
    "# 1. 配置信息\n",
    "##################################\n",
    "\n",
    "train_file = \"../data_train_Run.csv\"\n",
    "test_file = \"../data_test_Run.csv\"\n",
    "\n",
    "IGNORE_FEATURES = ['date','storeid','goodsid', 'sale_qty']\n",
    "CATEGORITAL_FEATURES = ['cost_flag', 'day_type', 'deptid',\n",
    "              'deptlevelid','name_up2', 'name_up3',\n",
    "              'name_up4', 'origin', 'ppname','AQI_level',\n",
    "              'DAY','MONTH', 'YEAR','spec', 'spname', 'stocktype', \n",
    "              'unitname', 'week', 'weekday','24term_name_cn','AQI_bin',\n",
    "              'TMP_bin','price_bin','DI_bin','pre1h_bin','sale_qty_bin']\n",
    "\n",
    "NUMERIC_FEATURES = ['AQI','DI',\n",
    "           'before_sale_qty_00',\n",
    "           'before_sale_qty_01', 'before_sale_qty_02',\n",
    "           'before_sale_qty_03', 'before_sale_qty_04', \n",
    "           'before_sale_qty_05','before_sale_qty_06', \n",
    "           'saletaxrate','pre1h', 'price', 'rhu','tem'\n",
    "            ]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "##################################\n",
    "# 2. 读取文件\n",
    "##################################\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "##################################\n",
    "# 3. 准备数据\n",
    "##################################\n",
    "\n",
    "# FeatureDict\n",
    "config.feature_dict, config.feature_size = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest, numeric_cols=NUMERIC_FEATURES, ignore_cols=IGNORE_FEATURES)\n",
    "\n",
    "# Xi, Xv\n",
    "Xi_train, Xv_train, y = parse(feat_dict=config.feature_dict, df=dfTrain, has_label=True)\n",
    "Xi_test, Xv_test, ids = parse(feat_dict=config.feature_dict, df=dfTest, has_label=False)\n",
    "config.field_size = len(Xi_train[0])\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "# 4. 建立模型\n",
    "##################################\n",
    "\n",
    "# 模型参数\n",
    "deep_layers = [32,32]\n",
    "config.embedding_size = 8\n",
    "config.deep_layers_activation = tf.nn.relu\n",
    "\n",
    "# BUILD THE WHOLE MODEL\n",
    "tf.set_random_seed(2018)\n",
    "\n",
    "\n",
    "# init_weight\n",
    "weights = dict()\n",
    "# Sparse Features 到 Dense Embedding的全连接权重。[其实是Embedding]\n",
    "weights['feature_embedding'] = tf.Variable(initial_value=tf.random_normal(shape=[config.feature_size, config.embedding_size],mean=0,stddev=0.1),\n",
    "                                           name='feature_embedding',\n",
    "                                           dtype=tf.float32)\n",
    "# Sparse Featues 到 FM Layer中Addition Unit的全连接。 [其实是Embedding,嵌入后维度为1]\n",
    "weights['feature_bias'] = tf.Variable(initial_value=tf.random_uniform(shape=[config.feature_size, 1],minval=0.0,maxval=1.0),\n",
    "                                      name='feature_bias',\n",
    "                                      dtype=tf.float32)\n",
    "# Hidden Layer\n",
    "num_layer = len(deep_layers)\n",
    "input_size = config.field_size * config.embedding_size\n",
    "glorot = np.sqrt(2.0 / (input_size + deep_layers[0])) # glorot_normal: stddev = sqrt(2/(fan_in + fan_out))\n",
    "weights['layer_0'] = tf.Variable(initial_value=tf.random_normal(shape=[input_size, deep_layers[0]],mean=0,stddev=glorot),\n",
    "                                 dtype=tf.float32)\n",
    "weights['bias_0'] = tf.Variable(initial_value=tf.random_normal(shape=[1, deep_layers[0]],mean=0,stddev=glorot),\n",
    "                                dtype=tf.float32)\n",
    "for i in range(1, num_layer):\n",
    "    glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "    # deep_layer[i-1] * deep_layer[i]\n",
    "    weights['layer_%d' % i] = tf.Variable(initial_value=tf.random_normal(shape=[deep_layers[i - 1], deep_layers[i]],mean=0,stddev=glorot),\n",
    "                                          dtype=tf.float32)\n",
    "    # 1 * deep_layer[i]\n",
    "    weights['bias_%d' % i] = tf.Variable(initial_value=tf.random_normal(shape=[1, deep_layers[i]],mean=0,stddev=glorot),\n",
    "                                         dtype=tf.float32)\n",
    "# Output Layer\n",
    "deep_size = deep_layers[-1]\n",
    "fm_size = config.field_size + config.embedding_size\n",
    "input_size = fm_size + deep_size\n",
    "glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "weights['concat_projection'] = tf.Variable(initial_value=tf.random_normal(shape=[input_size,1],mean=0,stddev=glorot),\n",
    "                                           dtype=tf.float32)\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(value=0.01), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# build_network\n",
    "feat_index = tf.placeholder(dtype=tf.int32, shape=[None, config.field_size], name='feat_index') # [None, field_size]\n",
    "feat_value = tf.placeholder(dtype=tf.float32, shape=[None, None], name='feat_value') # [None, field_size]\n",
    "label = tf.placeholder(dtype=tf.float16, shape=[None,1], name='label')\n",
    "\n",
    "# Sparse Features -> Dense Embedding\n",
    "embeddings_origin = tf.nn.embedding_lookup(weights['feature_embedding'], ids=feat_index) # [None, field_size, embedding_size]\n",
    "\n",
    "feat_value_reshape = tf.reshape(tensor=feat_value, shape=[-1, config.field_size, 1]) # -1 * field_size * 1\n",
    "\n",
    "# --------- 一维特征 -----------\n",
    "y_first_order = tf.nn.embedding_lookup(weights['feature_bias'], ids=feat_index) # [None, field_size, 1]\n",
    "w_mul_x = tf.multiply(y_first_order, feat_value_reshape) # [None, field_size, 1]  Wi * Xi\n",
    "y_first_order = tf.reduce_sum(input_tensor=w_mul_x, axis=2) # [None, field_size]\n",
    "\n",
    "# --------- 二维组合特征 ----------\n",
    "embeddings = tf.multiply(embeddings_origin, feat_value_reshape) # [None, field_size, embedding_size] multiply不是矩阵相乘，而是矩阵对应位置相乘。这里应用了broadcast机制。\n",
    "\n",
    "# sum_square part 先sum，再square\n",
    "summed_features_emb = tf.reduce_sum(input_tensor=embeddings, axis=1) # [None, embedding_size]\n",
    "summed_features_emb_square = tf.square(summed_features_emb)\n",
    "\n",
    "# square_sum part\n",
    "squared_features_emb = tf.square(embeddings)\n",
    "squared_features_emb_summed = tf.reduce_sum(input_tensor=squared_features_emb, axis=1) # [None, embedding_size]\n",
    "\n",
    "# second order\n",
    "y_second_order = 0.5 * tf.subtract(summed_features_emb_square, squared_features_emb_summed)\n",
    "\n",
    "\n",
    "# ----------- Deep Component ------------\n",
    "y_deep = tf.reshape(embeddings_origin, shape=[-1, config.field_size * config.embedding_size]) # [None, field_size * embedding_size]\n",
    "for i in range(0, len(deep_layers)):\n",
    "    y_deep = tf.add(tf.matmul(y_deep, weights['layer_%d' % i]), weights['bias_%d' % i])\n",
    "    y_deep = config.deep_layers_activation(y_deep)\n",
    "\n",
    "# ----------- output -----------\n",
    "concat_input = tf.concat([y_first_order, y_second_order, y_deep], axis=1)\n",
    "out = tf.add(tf.matmul(concat_input, weights['concat_projection']), weights['concat_bias'])\n",
    "out = tf.nn.sigmoid(out)\n",
    "\n",
    "config.loss = \"mse\"\n",
    "config.l2_reg = 0.1\n",
    "config.learning_rate = 0.1\n",
    "\n",
    "# loss\n",
    "if config.loss == \"logloss\":\n",
    "    loss = tf.losses.log_loss(label, out)\n",
    "elif config.loss == \"mse\":\n",
    "    loss = tf.losses.mean_squared_error(label, out)\n",
    "\n",
    "# l2\n",
    "if config.l2_reg > 0:\n",
    "    loss += tf.contrib.layers.l2_regularizer(config.l2_reg)(weights['concat_projection'])\n",
    "    for i in range(len(deep_layers)):\n",
    "        loss += tf.contrib.layers.l2_regularizer(config.l2_reg)(weights['layer_%d' % i])\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(loss)\n",
    "\n",
    "##################################\n",
    "# 5. 训练\n",
    "##################################\n",
    "\n",
    "# init session\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train\n",
    "feed_dict = {\n",
    "    feat_index: Xi_train,\n",
    "    feat_value: Xv_train,\n",
    "    label:      np.array(y).reshape((-1,1))\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    train_loss,opt = sess.run((loss, optimizer), feed_dict=feed_dict)\n",
    "    print(\"epoch: {0}, train loss: {1:.6f}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "# 6. 预测\n",
    "##################################\n",
    "dummy_y = [1] * len(Xi_test)\n",
    "feed_dict_test = {\n",
    "    feat_index: Xi_test,\n",
    "    feat_value: Xv_test,\n",
    "    label: np.array(dummy_y).reshape((-1,1))\n",
    "}\n",
    "\n",
    "prediction = sess.run(out, feed_dict=feed_dict_test)\n",
    "\n",
    "#sub = pd.DataFrame({\"\":ids, \"pred\":np.squeeze(prediction)})\n",
    "#print(\"prediction:\")\n",
    "#print(sub)\n",
    "print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
