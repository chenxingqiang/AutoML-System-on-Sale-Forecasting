"""ThinkPage Retail Forecasting refer modelversion 1 newdev:xingqiang chendate:2018-05-23"""# -*-coding:utf-8-*-import mathimport sysimport timefrom math import expimport argparseimport datetime as DTimport jsonimport numpy as npimport pandas as pdimport pymysqlfrom datetime import datefrom lib import faissfrom sklearn.preprocessing import LabelEncoderfrom sqlalchemy import create_enginedef parse_arguments(argv):    """    :param argv:    :return:    """    DATE = time.strftime('%Y-%m-%d', time.localtime(time.time()))    parser = argparse.ArgumentParser()    parser.add_argument('--DATE', type=str, help='the date of starting prediction.', default=DATE)    return parser.parse_args(argv)class Initialization(object):    """    Define The Global Variables and Constants    """    def __init__(self,                 date,                 pre_days=3,                 data_nbrs=800,                 train_nbr=730,                 val_nbr=36,                 before_job_days=9,                 before_weekends_days=8,                 LTDs=[1],                 target_name='sale_qty',                 target_storeid="'A035','A022','A013','A040'",                 model_name='XGBOOST',                 version='V10_test4',                 states='all'):        """        :param date: today's date ,string type like '2018-03-14'        :param pre_days: in {1,2,3,..14}        :param data_nbrs:max_min=1335 min = train_nbr + max{round(before_job_days/5)*7,round(before_weekends_days/2)*7}        :param train_nbr: train_nbr at least more than 60        :param val_nbr: always about round({0.25~0.1}*train_nbr)        :param LTD_weekends: (LTD_weekends/2)*7 <= train_nbr        :param LTD_job: (LTD_job/5) *7  <= train_nbr        :param before_job_days: (data_nbrs-train_nbr) >=round(before_job_days/5)) *7        :param before_weekends_days: (data_nbrs-train_nbr) >=round(before_weekends_days/2)*7        :param model_name: XGBOOST,KNN,RFR,ETR,GBDT，LightGBM        :param version: tell the version of code        :param states: choose the goods list for prediction: 'all','normal_24','special_4'        """        self.TODAY_DATE = date        print(u"##  当前日期：" + self.TODAY_DATE)        self.PRE_DAYS = pre_days        print(u"##  预测天数:" + str(self.PRE_DAYS))        self.DATA_START_DAY = self.FT(self.TODAY_DATE, - data_nbrs)        print(u"##  获取数据的长度{}天,起点日期：".format(data_nbrs) + self.DATA_START_DAY)        self.TRAIN_START_DAY = self.FT(self.TODAY_DATE, -train_nbr)        print(u"##  训练数据的长度:{}天,起点日期:".format(train_nbr) + self.TRAIN_START_DAY)        self.VAL_date = self.FT(self.TODAY_DATE, -val_nbr)        print(u"##  验证数据的起点日期：" + self.VAL_date)        self.PREDICTION_DAY = self.FT(self.TODAY_DATE, pre_days - 1)        print(u"##  预测日期：" + self.PREDICTION_DAY)        self.TRAIN_DATE = self.date_dt(self.TRAIN_START_DAY)        self.VAL_DATE = self.date_dt(self.VAL_date)        self.PRE_DATE = self.date_dt(self.TODAY_DATE)        self.LTD_weekends = round(train_nbr / 7 * 5)        self.LTD_job = round(train_nbr / 7 * 2)        self.before_job_days = before_job_days        print(u'##  工作日的序列长度:', self.before_job_days)        self.before_weekends_days = before_weekends_days        print(u'##  休息日的序列长度:', self.before_weekends_days)        self.LTDs = LTDs        print(u'##  训练集长度调整: ' + str([round(train_nbr / ltd) for ltd in LTDs]))        self.MODEL = model_name        print(u'##  预测的模型：', self.MODEL)        self.version = version        self.NAME_STAMP = model_name + '_' + version + '#' + self.PREDICTION_DAY        print(self.NAME_STAMP)        self.target_storeid = target_storeid        print('预测门店：', self.target_storeid)        self.states = states        print('预测集合：', self.states)        self.NAME_APP = """./Email/{0}_{1}#{2}_{3}_{4}_{5}_{6}_{7}_{8}_""".format(model_name, version + '_' + states,                                                                                  before_weekends_days,                                                                                  before_job_days,                                                                                  val_nbr, train_nbr, data_nbrs,                                                                                  date, pre_days)        self.tags_of_volume = ['date', 'storeid', 'goodsid', target_name]        self.tags_labels = ['date', 'storeid', 'goodsid']        self.tags_ts = ['storeid', 'goodsid', target_name]        self.tags_tiny = ['storeid', 'goodsid']        self.target_name = target_name    def T(self, FORMAT_DATE, i):        return (DT.datetime.strptime(FORMAT_DATE, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y%m%d')    def FT(self, FORMAT_DATE, i):        return (DT.datetime.strptime(FORMAT_DATE, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y-%m-%d')    def formatdate_a(self, d, i):        return (DT.datetime.strptime(d, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y-%m-%d')    def date_dt(self, DAY):        return date(int(DAY[0:4]), int(DAY[5:7]), int(DAY[8:]))    def condition_timeseries(self, condition, before_con_days, feature_name):        """        :param condition:        :param before_con_days:        :param feature_name:        :return:        """        # before_con_days = 3        dates = sorted(list(set(condition.date)), reverse=True)        con_all_day = condition[['storeid', 'date', feature_name]].drop_duplicates()        df_test = con_all_day.pivot_table(index=['storeid'], columns=['date'], values=[feature_name])        con_day_n = df_test[feature_name].sort_index(axis=1, ascending=False).reset_index()        Before_con_cols = ['before_' + feature_name + '_' + str(i).zfill(2) for i in range(before_con_days)]        before_0 = con_day_n[con_day_n.columns[-before_con_days:]]        before_0.columns = Before_con_cols[0:before_con_days]        before_title = con_day_n['storeid']        before_end_0 = pd.concat([before_title, before_0], axis=1)        before_end_0.insert(1, 'date', con_day_n.columns[-(before_con_days + 1)])        before_1 = con_day_n[con_day_n.columns[-(before_con_days + 1):-1]]        before_1.columns = Before_con_cols[0:before_con_days]        before_title = con_day_n['storeid']        before_end_1 = pd.concat([before_title, before_1], axis=1)        before_end_1.insert(1, 'date', con_day_n.columns[-(before_con_days + 2)])        before_data = pd.concat([before_end_0, before_end_1], axis=0)        for i in range(0, len(dates[1:-(before_con_days + 2)]) + 1):            before = con_day_n[con_day_n.columns[-(before_con_days + 2) - i:-2 - i]]            before.columns = Before_con_cols[0:before_con_days]            before_title = con_day_n['storeid']            before_end = pd.concat([before_title, before], axis=1)            before_end.insert(1, 'date', dates[-(before_con_days + 2) - i - 1])            # drop duplicates            before_end = before_end.drop_duplicates()            before_data = pd.concat([before_data, before_end], axis=0)        before_data = before_data.reset_index().drop('index', axis=1)        return before_data    def make_condition_distribution(self, condition_label, df_train_val_con):        """        :param condition_label: list type        :return:        """        # condition_label =['day_type']        condition_tag = ''        for item in condition_label:            condition_tag += item + '_'        price_distr = df_train_val_con.groupby(['goodsid'] + condition_label).apply(            lambda x: x['price'].abs().describe())        price_distr.columns = [condition_tag + 'price_' + col_nm for col_nm in price_distr.columns]        price_distr = price_distr.reset_index().fillna(0.0)        discount_distr = df_train_val_con.groupby(['goodsid'] + condition_label).apply(            lambda x: x['discount_degree'].abs().describe())        discount_distr.columns = [condition_tag + 'discount_' + col_nm for col_nm in discount_distr.columns]        discount_distr = discount_distr.reset_index().fillna(0.0)        saleqty_distr = df_train_val_con.groupby(['goodsid'] + condition_label).apply(            lambda x: x['sale_qty'].abs().describe())        saleqty_distr.columns = [condition_tag + 'saleqty_' + col_nm for col_nm in saleqty_distr.columns]        saleqty_distr = saleqty_distr.reset_index().fillna(0.0)        cost_distr = df_train_val_con.groupby(['goodsid'] + condition_label).apply(            lambda x: x['cost'].abs().describe())        cost_distr.columns = [condition_tag + 'cost_' + col_nm for col_nm in cost_distr.columns]        cost_distr = cost_distr.reset_index().fillna(0.0)        purchaseqty_distr = df_train_val_con.groupby(['goodsid'] + condition_label).apply(            lambda x: x['purchase_qty'].abs().describe())        purchaseqty_distr.columns = [condition_tag + 'purchaseqty_' + col_nm for col_nm in                                     purchaseqty_distr.columns]        purchaseqty_distr = purchaseqty_distr.reset_index().fillna(0.0)        return price_distr, discount_distr, discount_distr, saleqty_distr, cost_distr, purchaseqty_distr    def make_time_series(self, data, before_days, target_label='sale_qty'):        """        :param data:        :param before_days:        :param target_label:        :return:        """        print(u"""制作 """ + target_label + u""" 时间序列....中""")        tags = self.tags_labels + [target_label]        dates = sorted(list(set(data.date)), reverse=True)        df_all_day = data[tags]        df_test = df_all_day.pivot_table(index=['storeid', 'goodsid'], columns=['date'],                                         values=[target_label])        df_day_n = df_test[target_label].sort_index(axis=1, ascending=False).reset_index().fillna(0.0)        if self.before_job_days > self.before_weekends_days:            Before_cols = ['before_' + target_label + '_' + str(i).zfill(2) for i in                           range(0, self.before_job_days)]        else:            Before_cols = ['before_' + target_label + '_' + str(i).zfill(2) for i in range(0,                                                                                           self.before_weekends_days)]        before_0 = df_day_n[df_day_n.columns[-before_days:]]        before_0.columns = Before_cols[0:before_days]        before_title = df_day_n[self.tags_tiny]        before_end_0 = pd.concat([before_title, before_0], axis=1)        before_end_0.insert(2, 'date', df_day_n.columns[-(before_days + 1)])        before_end_0 = before_end_0.drop_duplicates()        before_1 = df_day_n[df_day_n.columns[-(before_days + 1):-1]]        before_1.columns = Before_cols[0:before_days]        before_title = df_day_n[self.tags_tiny]        before_end_1 = pd.concat([before_title, before_1], axis=1)        before_end_1.insert(2, 'date', df_day_n.columns[-(before_days + 2)])        ## drop duplicates        before_end_1 = before_end_1.drop_duplicates()        before_data = pd.concat([before_end_0, before_end_1], axis=0)        for i in range(0, len(dates[1:-(before_days + 2)]) + 1):            before = df_day_n[df_day_n.columns[-(before_days + 2) - i:-2 - i]]            before.columns = Before_cols[0:before_days]            before_title = df_day_n[self.tags_tiny]            before_end = pd.concat([before_title, before], axis=1)            before_end.insert(1, 'date', dates[-(before_days + 2) - i - 1])            ## drop duplicates            before_end = before_end.drop_duplicates()            before_data = pd.concat([before_data, before_end], axis=0)        before_data = before_data.reset_index().drop('index', axis=1)        before_data = before_data.drop_duplicates(['date', 'goodsid', 'storeid'])        return before_dataclass Get_Mysql_Data(object):    def __init__(self, sudoer, database='CHAOSHIFA_DB', host='47.97.17.51', user='sophon',                 password='IAmSophon_4523', port=3306):        print(database)        self.database = database        self.host = host        self.user = user        self.password = password        self.port = port        self.DATA_START_DAY = sudoer.DATA_START_DAY        self.TODAY_DATE = sudoer.TODAY_DATE        self.target_storeid = sudoer.target_storeid    def parse(self, cols, sql):        """        :params:cols,list type        :params:sql_command,string type        :return: df,Dataframe type        """        print(cols, sql)        db = pymysql.connect(self.host, self.user, self.password, self.database, self.port, charset='utf8')        cursor = db.cursor()        cursor.execute(sql)        results = cursor.fetchall()        df = pd.DataFrame(list(results), columns=cols)        return df    def get_df_actual_purchase_his(self):        """        """        # purchase data from db        cols = ['date', 'storeid', 'goodsid', 'cost', 'purchase_qty']        sql_command = """                        SELECT                        date,storeid, goodsid,cost,qty                        FROM                        mid_aggrbydate_actualpurchase_his                        WHERE date >='{start_day}'                        AND storeid in ({target_storeid})                        AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE,                                 target_storeid=self.target_storeid)        df_actual_purchase_his = self.parse(cols, sql)        return df_actual_purchase_his    def get_df_aggrbydate_airq_his(self):        """        """        # airq data from db        cols = ['date', 'storeid', 'AQI']        sql_command = """                        SELECT                         date,storeid, AQI                         FROM                          mid_aggrbydate_airq_his                         WHERE date >='{start_day}'                        AND storeid in ({target_storeid})                        AND date<'{today_date}'                        """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE,                                 target_storeid=self.target_storeid)        df_aggrbydate_airq = self.parse(cols, sql)        return df_aggrbydate_airq    def get_df_aggrbydate_saleprice(self):        """        """        # price data from db        cols = ['date', 'storeid', 'goodsid', 'discount_degree', 'price']        sql_command = """                        SELECT                        date,storeid,goodsid,discount_degree,price                         FROM                        mid_aggrbydate_saleprice_his                         WHERE date >='{start_day}'                        AND storeid in ({target_storeid})                        AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE,                                 target_storeid=self.target_storeid)        df_aggrbydate_saleprice = self.parse(cols, sql)        return df_aggrbydate_saleprice    def get_df_aggrbydate_saleqty(self):        """        """        # price data from db        cols = ['date', 'storeid', 'goodsid', 'sale_qty']        sql_command = """                        SELECT                         date,storeid,goodsid,sale_qty                         FROM                        mid_aggrbydate_saleqty_his                         WHERE date >='{start_day}'                         AND storeid in ({target_storeid})                        AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE,                                 target_storeid=self.target_storeid)        df_aggrbydate_saleqty = self.parse(cols, sql)        return df_aggrbydate_saleqty    def get_df_aggrbydate_weather_his(self):        """        """        # price data from db        cols = ['date', 'storeid', 'tem', 'windspeed', 'pre1h', 'rhu']        sql_command = """                        SELECT                         date,storeid,tem,windspeed,pre1h,rhu                         FROM                        mid_aggrbydate_weather_his                         WHERE date >='{start_day}'                        AND storeid in ({target_storeid})                        AND date<'{today_date}'                        """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE,                                 target_storeid=self.target_storeid)        df_aggrbydate_weather_his = self.parse(cols, sql)        return df_aggrbydate_weather_his    def get_df_category_his(self):        """        """        # category data from db        cols = ['deptid', 'name', 'name_up1', 'name_up2',                'name_up3', 'name_up4', 'deptlevelid', 'date']        sql_command = """                        SELECT                         deptid,name,name_up1,name_up2,                        name_up3,name_up4, deptlevelid, date                         FROM                        raw_cleaned_category_his                         WHERE date ='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df_category_his = self.parse(cols, sql)        return df_category_his    def get_df_goods_his(self):        """        """        # category data from db        cols = ['goodsid', 'spname', 'ppname', 'deptid',                'goodstypeid', 'stocktype', 'saletaxrate',                'spec', 'unitname', 'origin', 'keepdays',                'indate', 'date']        sql_command = """                        SELECT                         goodsid, spname,ppname,deptid,goodstypeid,stocktype,                        saletaxrate,spec,unitname,origin,keepdays,indate,date                         FROM                          raw_cleaned_goods_his                         WHERE date ='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df_goods_his = self.parse(cols, sql)        return df_goods_his    def get_df_canlendar_info(self):        """        """        # category data from db        cols = ['date', 'weekday', '24term_name_en',                '24term_code', '24term_name_cn', 'holiday_bylaw']        sql_command = """                        SELECT                         date, weekday,24term_name_en,                        24term_code,24term_name_cn,holiday_bylaw                         FROM                         calendar                         WHERE date >='{start_day}'                         """        sql = sql_command.format(start_day=self.DATA_START_DAY)        df_canlendar_info = self.parse(cols, sql)        return df_canlendar_info    def get_df_actualpurchase_fcst(self):        """        """        cols = ['date', 'goodsid', 'cost', 'storeid']        sql_command = """                        SELECT                        date,goodsid,cost,storeid                         FROM                        mid_aggrbydate_actualpurchase_fcst                        WHERE date>='{today_date}'                        AND date_cur='{today_date}'                        AND storeid in ({target_storeid})                        """        sql = sql_command.format(today_date=self.TODAY_DATE, target_storeid=self.target_storeid)        df = self.parse(cols, sql)        return df    def get_df_weather_fcst(self):        """        """        cols = ['date', 'storeid', 'tem', 'windspeed', 'pre1h', 'rhu']        sql_command = """                        SELECT                         date,storeid,tem,windspeed,pre1h,rhu                        FROM                         mid_aggrbydate_weather_fcst                        WHERE date>='{today_date}'                         AND date_cur='{today_date}'                        AND storeid in ({target_storeid})                    """        sql = sql_command.format(today_date=self.TODAY_DATE, target_storeid=self.target_storeid)        df = self.parse(cols, sql)        return df    def get_df_airq_fcst(self):        """        """        cols = ['date', 'storeid', 'AQI']        sql_command = """                        SELECT date,storeid, aqi                         FROM                         mid_aggrbydate_airq_fcst                         WHERE date_cur='{today_date}'                        AND storeid in ({target_storeid})                        """        sql = sql_command.format(today_date=self.TODAY_DATE, target_storeid=self.target_storeid)        df = self.parse(cols, sql)        return df    def get_market_campaign(self):        """        """        # price data from db        cols = ['goodsid', 'storeid', 'daterange', 'campaign_type_cn', 'campaign_type_en']        sql = """                SELECT                 goodsid,storeid,date_range,campaign_type_en,campaign_type_en                FROM                  market_campaign                WHERE storeid in ({target_storeid})             """        sql = sql.format(target_storeid=self.target_storeid)        df_x = self.parse(cols, sql)        def data_deaf(x, z, l):            m = l.split('_')[0]            n = l.split('_')[1]            a = [str(a).split(' ')[0] for a in pd.date_range(m, n)]            y = pd.DataFrame(a, columns=['date'])            y['goodsid'] = x            y['campaign_type_en'] = z            return y        flag = 0        for item in df_x.values:            if len(item) == 0:                print('XXXX')                break            else:                x = data_deaf(item[0], item[4], item[2])                if flag == 0:                    data_temp = x                    flag = 1                else:                    data_temp = pd.concat([data_temp, x], axis=0)        df = data_temp.reset_index().drop('index', axis=1)        le = LabelEncoder()        for col_name in ['campaign_type_en']:            df[col_name] = le.fit_transform(df[col_name].values)        return df    def df_get_campaign_his(self):        """        """        cols = ['date', 'storeid', 'goodsid', 'campaign_nm_en', 'date_range', 'effect_est']        sql = """                SELECT                date,storeid,goodsid,campaign_nm_en,date_range,effect_est                FROM                mid_aggrbydate_campaign_forfeature                WHERE storeid in ({target_storeid})                """        sql = sql.format(target_storeid=self.target_storeid)        df = self.parse(cols, sql)        return df    def get_price_future(self):        """        """        # price data from db        cols = ['storeid', 'goodsid', 'price_str', 'date_range', 'rule_name']        sql = """                SELECT                 storeid,goodsid,price_str,date_range,rule_name                FROM                  raw_price_info_future                WHERE storeid in ({target_storeid})             """        sql = sql.format(target_storeid=self.target_storeid)        df_x = self.parse(cols, sql)        def data_deaf(storeid, goodsid, price_str, date_range, rule_name):            m = date_range.split('_')[0]            n = date_range.split('_')[1]            a = [str(a).split(' ')[0] for a in pd.date_range(m, n)]            y = pd.DataFrame(a, columns=['date'])            y['storeid'] = storeid            y['goodsid'] = goodsid            y['price_ordinary'] = float(price_str.split('@')[0]) / 1000.0            y['price_custom'] = float(price_str.split('@')[-1]) / 1000.0            y['rule_name'] = rule_name            return y        flag = 0        for item in df_x.values:            if len(item) == 0:                print('XXXX')                break            else:                x = data_deaf(item[0], item[1], item[2], item[3], item[4])                if flag == 0:                    data_temp = x                    flag = 1                else:                    data_temp = pd.concat([data_temp, x], axis=0)        df = data_temp.reset_index().drop('index', axis=1)        le = LabelEncoder()        for col_name in ['rule_name']:            df[col_name] = le.fit_transform(df[col_name].values)        return df    def get_mid_aggrbydate_num_of_pay_vege_data(self):        """        果蔬类相关的小票个数的表：        mid_aggrbydate_num_of_pay_vege        """        cols = ["storeid", "date", "payvalue", "num_pay"]        sql_command = """                        SELECT *                         FROM mid_aggrbydate_num_of_pay                        WHERE storeid in ({target_storeid})                        """        sql = sql_command.format(target_storeid=self.target_storeid)        df = self.parse(cols, sql)        return dfclass Get_Training_Data():    """    """    def __init__(self, sudoer, promotion, features_info, storage_info, df_train_pre):        self.df_train_pre = df_train_pre        self.condition = features_info['condition']        self.items_info = features_info['items_info']        self.before_weekends_days = sudoer.before_weekends_days        self.before_job_days = sudoer.before_job_days        self.TRAIN_START_DAY = sudoer.TRAIN_START_DAY        self.make_time_series = sudoer.make_time_series        self.promotion_info = promotion['data_market_campin']        self.pay_info = features_info['pay_info']        self.goods_statistic_dt = pd.concat([features_info['dt_price_distr'].set_index(['goodsid', 'day_type']                                                                                       ),                                             features_info['dt_discount_distr'].set_index(                                                 ['goodsid', 'day_type']),                                             features_info['dt_saleqty_distr'].set_index(                                                 ['goodsid', 'day_type']),                                             features_info['dt_cost_distr'].set_index(                                                 ['goodsid', 'day_type']),                                             features_info['dt_purchaseqty_distr'].set_index(                                                 ['goodsid', 'day_type'])                                             ], axis=1).reset_index()        self.goods_statistic_gy = pd.concat([features_info['gy_price_distr'].set_index(['goodsid']),                                             features_info['gy_discount_distr'].set_index(                                                 ['goodsid']),                                             features_info['gy_saleqty_distr'].set_index(                                                 ['goodsid']),                                             features_info['gy_cost_distr'].set_index(['goodsid']),                                             features_info['gy_purchaseqty_distr'].set_index(                                                 ['goodsid'])                                             ], axis=1).reset_index()        self.goods_statistic_aqi = pd.concat([features_info['aqi_price_distr'].set_index(['goodsid',                                                                                          'AQI_level']                                                                                         ),                                              features_info['aqi_discount_distr'].set_index(                                                  ['goodsid', 'AQI_level']),                                              features_info['aqi_saleqty_distr'].set_index(                                                  ['goodsid', 'AQI_level']),                                              features_info['aqi_cost_distr'].set_index(                                                  ['goodsid', 'AQI_level']),                                              features_info['aqi_purchaseqty_distr'].set_index(                                                  ['goodsid', 'AQI_level'])                                              ], axis=1).reset_index()        self.target_name = sudoer.target_name        self.storage_info = pd.concat([storage_info['gy_storage0_distr'].set_index('goodsid'),                                       storage_info['gy_storage1_distr'].set_index('goodsid'),                                       storage_info['gy_storage2_distr'].set_index('goodsid')                                       ], axis=1).reset_index()    def make_data_two_part(self, df_train_pre):        """        :return:        """        history_data = df_train_pre        history_data = history_data[history_data.date >= self.TRAIN_START_DAY]        print("""LR 函数归一化 销量""")        history_data['sale_qty'] = history_data['sale_qty'].map(            lambda x: np.log1p(float(x)) if float(x) > 0 else 0)        history_data['week'] = history_data['date'].map(            lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))        print("""数据集划分两部分 按照工作日 和非工作日""")        df_0 = history_data[history_data['week'] == 0]        df_1 = history_data[history_data['week'] == 6]        history_weekends = pd.concat([df_0, df_1], axis=0)        history_job = history_data[0 < history_data['week']]        history_job = history_job[history_job['week'] < 6]        history_weekends.sort_values(['date'], inplace=True, ascending=True)        history_job.sort_values(['date'], inplace=True, ascending=True)        return history_weekends, history_job    def pre_features_for_separate_data(self):        print(u'特征准备')        df_train_pre = self.df_train_pre        history_weekends, history_job = self.make_data_two_part(df_train_pre)        before_data_w = self.make_time_series(history_weekends, self.before_weekends_days)        before_data_j = self.make_time_series(history_job, self.before_job_days)        # before_cost_w = self.make_time_series( history_weekends , self.before_weekends_days , 'cost' )        # before_cost_j = self.make_time_series( history_job , self.before_job_days , 'cost' )        before_price_w = self.make_time_series(history_weekends, self.before_weekends_days, 'price')        before_price_j = self.make_time_series(history_job, self.before_job_days, 'price')        # before_purchase_j = self.make_time_series( history_job , self.before_job_days , 'purchase_qty' )        # before_purchase_w = self.make_time_series( history_weekends , self.before_weekends_days , 'purchase_qty' )        print(u'特征结合')        features_w = pd.merge(before_data_w, history_weekends, how='left',                              on=['date', 'storeid', 'goodsid', ])        features_j = pd.merge(before_data_j, history_job, how='left',                              on=['date', 'storeid', 'goodsid'])        features_j['week'] = features_j['date'].map(            lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))        features_w['week'] = features_w['date'].map(            lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))        ### make training and prediction dataset        week_one_hot_j = pd.get_dummies(features_j.week, 'week')        ### make training and prediction dataset        week_one_hot_w = pd.get_dummies(features_w.week, 'week')        features_w = pd.concat([features_w, week_one_hot_w], axis=1)        features_j = pd.concat([features_j, week_one_hot_j], axis=1)        # features_w = pd.merge( before_purchase_w , features_w , how='left' ,        # on=[ 'date' , 'storeid' , 'goodsid' , ] )        # features_j = pd.merge( before_purchase_j , features_j , how='left' ,        # on=[ 'date' , 'storeid' , 'goodsid' ] )        # features_w = pd.merge( before_cost_w , features_w , how='left' ,        # on=[ 'date' , 'storeid' , 'goodsid' ] )        # features_j = pd.merge( before_cost_j , features_j , how='left' ,        # on=[ 'date' , 'storeid' , 'goodsid' ] )        features_w = pd.merge(before_price_w, features_w, how='left',                              on=['date', 'storeid', 'goodsid'])        features_j = pd.merge(before_price_j, features_j, how='left',                              on=['date', 'storeid', 'goodsid'])        print("""结合促销信息：促销效果用 Posion 分布表示""")        features_w = pd.merge(features_w, self.promotion_info,                              on=['date', 'goodsid', 'storeid'], how='left')        features_w['onpromotion'] = features_w['discount_degree'].map(            lambda x: 1 if x > 0 else 0).astype(bool)        print("""结合促销信息：促销效果用 Posion 分布表示""")        features_j = pd.merge(features_j, self.promotion_info,                              on=['date', 'goodsid', 'storeid'], how='left')        features_j['onpromotion'] = features_j['discount_degree'].map(            lambda x: 1 if x > 0 else 0).astype(bool)        print("""工作日 结合 时间信息的特征：年 月 日 周几""")        features_j['DAY'] = features_j['date'].map(            lambda x: x.split('-')[-1]).astype(int)        features_j['MONTH'] = features_j['date'].map(            lambda x: x.split('-')[1]).astype(int)        features_j['YEAR'] = features_j['date'].map(            lambda x: x.split('-')[0]).astype(int)        print("""非工作日 结合 时间信息的特征：年 月 日 周几""")        features_w['DAY'] = features_w['date'].map(            lambda x: x.split('-')[-1]).astype(int)        features_w['MONTH'] = features_w['date'].map(            lambda x: x.split('-')[1]).astype(int)        features_w['YEAR'] = features_w['date'].map(            lambda x: x.split('-')[0]).astype(int)        print("add cost flag for weekends")        features_w['cost_flag'] = features_w['cost'].map(lambda x: 1 if x > 0 else 0)        print("add cost flag for work days")        features_j['cost_flag'] = features_j['cost'].map(lambda x: 1 if x > 0 else 0)        print("add payvalue and number of pay times for weekends")        features_w = pd.merge(features_w, self.pay_info, on=['storeid', 'date'], how='left')        print("add payvalue and number of pay times for work days")        features_j = pd.merge(features_j, self.pay_info, on=['storeid', 'date'], how='left')        features_w = pd.merge(features_w, self.condition, on=['storeid', 'date'], how='left')        features_w = pd.merge(features_w, self.items_info.reset_index(), on=['goodsid'], how='left')        features_w = pd.merge(features_w, self.goods_statistic_dt, on=['goodsid', 'day_type'], how='left')        features_w = pd.merge(features_w, self.goods_statistic_aqi, on=['goodsid', 'AQI_level'], how='left')        features_w = pd.merge(features_w, self.goods_statistic_gy, on=['goodsid', ], how='left')        features_w = pd.merge(features_w, self.storage_info, on=['goodsid'], how='left')        features_j = pd.merge(features_j, self.condition, on=['storeid', 'date'], how='left')        features_j = pd.merge(features_j, self.items_info.reset_index(), on=['goodsid'], how='left')        features_j = pd.merge(features_j, self.goods_statistic_dt, on=['goodsid', 'day_type'], how='left')        features_j = pd.merge(features_j, self.goods_statistic_aqi, on=['goodsid', 'AQI_level'], how='left')        features_j = pd.merge(features_j, self.goods_statistic_gy, on=['goodsid'], how='left')        features_j = pd.merge(features_j, self.storage_info, on=['goodsid'], how='left')        features_end_weekends = features_w.set_index(            ['date', 'storeid', 'goodsid', 'sale_qty']).reset_index().drop(['level_0', 'purchase_qty'],                                                                           axis=1)        features_end_job = features_j.set_index(['date', 'storeid', 'goodsid', 'sale_qty']).reset_index().drop(            ['level_0', 'purchase_qty'], axis=1)        # features_end_job.to_csv( 'features_end_job.csv' )        return {            'features_weekends': features_end_weekends,            'features_job': features_end_job        }class Pre_Data(object):    def __init__(self, sudoer, data_sql):        """        :param sudoer:        :param data_sql:        """        self.data_sql = data_sql        self.FT = sudoer.FT        self.TODAY_DATE = sudoer.TODAY_DATE        self.PRE_DAYS = sudoer.PRE_DAYS        self.NAME_APP = sudoer.NAME_APP        self.condition_timeseries = sudoer.condition_timeseries        self.make_condition_distribution = sudoer.make_condition_distribution    def pre_promotion(self):        """        :return:        """        def myfact(n):            """            """            n = int(n)            if n > 20:                n = 20            if n < 2:                return 1            else:                return n * myfact(n - 1)        flag = 0        gy_market_campaign = self.data_sql['new_market_campaign_his'].groupby(            ['storeid', 'goodsid', 'date_range'])        for name, group in gy_market_campaign:            if len(group) <= 7:                date_length = int(len(group) * 3 / 2)            else:                date_length = len(group)            m = group.effect_est.min()            start_date = group.date.values[0]            dates_add = pd.DataFrame([self.FT(start_date, i) for i in range(date_length)], columns=['date'])            dates_add = dates_add.reset_index()            dates_add['storeid'] = 'A035'            dates_add['goodsid'] = group.goodsid.values[0]            dates_add['campaign_nm_en'] = group.campaign_nm_en.values[0]            dates_add['date_range_add'] = start_date + '_' + self.FT(start_date, date_length)            dates_add['date_range'] = group.date_range.values[0]            dates_add['idx_m'] = dates_add['index'].map(lambda x: m ** x)            dates_add['idx_E'] = dates_add['index'].apply(myfact)            ## 泊松分布            dates_add['Poisson_distri'] = dates_add['idx_m'] / dates_add['idx_E']            dates_add['Poisson_distri'] = dates_add['Poisson_distri'].map(lambda x: x * exp(-m))            if flag == 0:                data_app = dates_add                flag = 1            else:                data_app = data_app.append(dates_add)        data_market_campin = data_app.reset_index().drop(['index', 'idx_E', 'idx_m'], axis=1)        le = LabelEncoder()        for col_name in ['campaign_nm_en', 'date_range_add', 'date_range']:            if data_market_campin[[col_name]].dtypes[0] == 'object':                data_market_campin[col_name] = le.fit_transform(data_market_campin[col_name].values)        promotion = {            'data_market_campin': data_market_campin        }        return promotion    def price_goods_match(self):        """        make price list        make for what goods we will predict        """        data_sql = self.data_sql        ## 容错模板：使用过去30天的 sale price history data.        latest_day30_price = data_sql['saleprice_his'][data_sql['saleprice_his'].date >= self.FT(            self.TODAY_DATE, -30)].drop('date', axis=1)        latest_day1_price = data_sql['saleprice_his'][data_sql['saleprice_his'].date == self.FT(            self.TODAY_DATE, -1)].drop('date', axis=1)        latest_price_choose = latest_day30_price.groupby(['storeid', 'goodsid']).apply(            lambda x: x['price'].abs().describe())        latest_price_choose.columns = ["latest_30days_price_" + column for column in                                       list(latest_price_choose.columns)]        latest_price_choose = latest_price_choose.reset_index()        latest_price = pd.merge(latest_day1_price, latest_price_choose, on=['storeid', 'goodsid'], how='right')        print(u'价格补缺 30日平均值策略：...\n')        latest_price['price_str'] = latest_price['price'].astype(str) + '_' + latest_price[            'latest_30days_price_mean'].astype(str)        latest_price['price'] = latest_price['price_str'].map(            lambda x: x.split('_')[1] if x.split('_')[0] == 'nan' else x.split('_')[0])        # TODO send mail as attachment        latest_price.to_csv(self.NAME_APP + 'latest_price.csv')        latest_price_2model = latest_price[['storeid', 'goodsid', 'price', 'discount_degree']]        # TODO send mail as attachment        latest_price_2model.to_csv(self.NAME_APP + 'latest_price_2model.csv')        print('该策略只负责过去三十日中可以通过平均价格策略补齐的价格，剩下未补足的通过判别器和生成器统一处理。\n')        dates = sorted(list(set(data_sql['actual_purchase_fcst'].date)))        print(u'判断预测日期数目是否足够:\n')        if len(dates) - self.PRE_DAYS == 0:            print('check is OK ! \n')            dates = sorted(list(set(data_sql['actual_purchase_fcst'].date)))        else:            dates = [self.FT(self.TODAY_DATE, i) for i in range(self.PRE_DAYS)]            print(u"预测日数据在数据库中获取不完整，启动备选方案，本次启动将通过邮件形式发送到管理员，请及时查收。\n")            # TODO            # send massage('预测日期不够，未来的信息表需要维护。')            print(u'预测日期不够，未来的信息表需要维护。模型已采取备选方案进行训练，正常情况下不影响预测。\n')        print(u'用历史30天的出现的 商品作为未来预测的基准，如果未来信息中有新的商品，将通过后续步骤补入。\n')        print(dates)        latest_prices = pd.DataFrame()        for day in dates:            temp = latest_price_2model.copy()            temp['date'] = day            latest_prices = latest_prices.append(temp)            del temp        latest_prices.to_csv(self.NAME_APP + 'latest_prices.csv')        latest_prices = latest_prices.reset_index().drop(['index'], axis=1)        df_fcst_price = data_sql['actual_purchase_fcst'].drop('cost', axis=1)        print(u'用未来信息补入可能出现的新的商品,如果存在疑问，请查询邮件附件进行比较可得出结论。\n')        print("## 目前 我们这个逻辑先固定用过去三十天出现的商品。暂且不考虑订货表中未来出现的新品。\n")        price_fcst = pd.merge(df_fcst_price, latest_prices, how='right', on=['goodsid', 'date', 'storeid'])        print(u'新商品的个数：', (len(price_fcst) - len(latest_prices)) / self.PRE_DAYS)        # TODO: send this file as attachment by email.        price_fcst.to_csv(self.NAME_APP + 'price_fcst.csv')        # 1.add the booked price  2. change the price        price_future = data_sql['price_future'][['date', 'storeid', 'goodsid', 'price_ordinary']]        price_fcst_future = pd.merge(price_fcst, price_future, how='left',                                     on=['date', 'storeid', 'goodsid']).fillna(0.0)        price_fcst_future['price_str'] = price_fcst_future.price.astype(            str) + '_' + price_fcst_future.price_ordinary.astype(str)        price_fcst_future = price_fcst_future.drop(['price'], axis=1)        price_fcst_future['price'] = price_fcst_future['price_str'].map(            lambda x: float(x.split('_')[0]) if float(x.split('_')[-1]) == 0.0 else float(                x.split('_')[-1                ]))        price_fcst = price_fcst_future[['date', 'storeid', 'goodsid', 'discount_degree', 'price']]        price_fcst.to_csv(self.NAME_APP + 'price_fcst_change_add.csv')        sale_price = pd.concat([data_sql['saleprice_his'], price_fcst], axis=0).sort_values(            by=['date']).set_index(['date', 'storeid', 'goodsid'])        ##TODO: send email        sale_price.to_csv(self.NAME_APP + 'sale_price_his_pre.csv')        return sale_price    def pre_features(self):        """        :return:        """        data_sql = self.data_sql        ## TODO: consider the campaign info for conditional analysis method        campaign_info_old = data_sql['market_campaign_his']        airq = data_sql['airq_his'].append(data_sql['airq_fcst']).set_index(['storeid', 'date'])        weather = data_sql['weather_his'].append(data_sql['weather_fcst']).set_index(['storeid', 'date'])        canlendar_info = data_sql['canlendar_info']        airq_weather = pd.concat([airq, weather], axis=1).fillna(0.0).reset_index()        airq_weather_canlendar = pd.merge(airq_weather, canlendar_info, how='left', on=['date']).fillna(            'Ordinary_day').drop(            ['24term_name_en', '24term_code'], axis=1)        airq_weather_canlendar['day_type'] = airq_weather_canlendar['weekday'].map(            lambda x: 'work_day' if 1 <= x <= 5 else 'weekends')        def AQI_LEVEL(x):            """            """            if 0.0 <= x <= 50.0:                return 'I'            elif 50.0 < x <= 100.0:                return 'II'            elif 100.0 < x <= 200.0:                return 'III'            elif 200.0 < x <= 300.0:                return 'IV'            elif x > 300.0:                return 'V'        airq_weather_canlendar['AQI_level'] = airq_weather_canlendar['AQI'].apply(AQI_LEVEL)        # make goods info        category_his = data_sql['category_his'].drop(['date'], axis=1)        goods_his = data_sql['goods_his'].drop(['indate', 'date'], axis=1)        items_info = pd.merge(category_his, goods_his, on=['deptid'], how='right')        items_info = items_info.set_index('goodsid')        items_info_origin = items_info.copy()        le = LabelEncoder()        for col_name in items_info.columns:            if items_info[[col_name]].dtypes[0] == 'object':                items_info[col_name] = le.fit_transform(items_info[col_name].values)        for col_name in airq_weather_canlendar.columns:            if airq_weather_canlendar[[col_name]].dtypes[0] == 'object' and \                    col_name != 'storeid' and col_name != 'date':                airq_weather_canlendar[col_name] = le.fit_transform(airq_weather_canlendar[col_name].values)        condition = airq_weather_canlendar        condition['date'] = condition['date'].astype(str)        before_tem = self.condition_timeseries(condition, 14, 'tem').set_index(['date', 'storeid'])        before_aqi = self.condition_timeseries(condition, 3, 'AQI').set_index(['date', 'storeid'])        before_pre1h = self.condition_timeseries(condition, 3, 'pre1h').set_index(['date', 'storeid'])        before_daytype = self.condition_timeseries(condition, 7, 'day_type').set_index(['date', 'storeid'])        before_windspeed = self.condition_timeseries(condition, 3, 'windspeed').set_index(['date', 'storeid'])        condition = pd.concat(            [condition.set_index(['date', 'storeid']), before_tem, before_aqi, before_pre1h,             before_daytype,             before_windspeed],            axis=1).reset_index().fillna(0.0)        saleqty = data_sql['saleqty'].set_index(['date', 'storeid', 'goodsid'])        sale_price = self.price_goods_match()        purchase = pd.concat([data_sql['actual_purchase_his'], data_sql['actual_purchase_fcst']]).set_index(            ['date', 'storeid', 'goodsid'])        df_train_pre_temp = pd.concat([saleqty, sale_price, purchase], axis=1,                                      join_axes=[sale_price.index]).fillna(0.0                                                                           ).reset_index()        df_train_pre = df_train_pre_temp.copy()        df_train_pre['sale_qty'] = df_train_pre['sale_qty'].map(            lambda x: x if x < 50 else x / 1000.0)        df_train_val = df_train_pre[df_train_pre.date < self.TODAY_DATE]        dfx_tag = pd.DataFrame(df_train_val[['goodsid', 'storeid',                                             'sale_qty']].groupby(            ['storeid', 'goodsid']).sale_qty.mean()).reset_index()        dfx_tag['X'] = dfx_tag['sale_qty'].map(lambda x: 1 if x < 50 else 1000)        dfx_tag = dfx_tag.drop(['sale_qty'], axis=1)        # TODO :try adding more condition type as if you are willing to do more distribution features        df_train_val_con = pd.merge(df_train_val, condition, on=['date', 'storeid'], how='left')        df_train_val_con_camp = pd.merge(df_train_val_con, campaign_info_old, on=['date', 'goodsid'],                                         how='left')        # df_train_val_con_camp.to_csv( 'df_train_val_con_camp.csv' )        gy_price_distr, gy_discount_distr, gy_discount_distr, gy_saleqty_distr, gy_cost_distr, \        gy_purchaseqty_distr = self.make_condition_distribution([], df_train_val_con)        dt_price_distr, dt_discount_distr, dt_discount_distr, dt_saleqty_distr, dt_cost_distr, \        dt_purchaseqty_distr = self.make_condition_distribution(['day_type'], df_train_val_con)        aqi_price_distr, aqi_discount_distr, aqi_discount_distr, aqi_saleqty_distr, aqi_cost_distr, \        aqi_purchaseqty_distr = self.make_condition_distribution(['AQI_level'], df_train_val_con)        # add py numbers each days        data_sql['aggrbydate_pay_his'][['date', 'storeid']] = data_sql['aggrbydate_pay_his'][            ['date', 'storeid']].astype(str)        pay_info = data_sql['aggrbydate_pay_his']        features_info = {            'condition': condition,            'items_info': items_info,            'items_info_origin': items_info_origin,            'gy_price_distr': gy_price_distr,            'gy_discount_distr': gy_discount_distr,            'gy_saleqty_distr': gy_saleqty_distr,            'gy_cost_distr': gy_cost_distr,            'gy_purchaseqty_distr': gy_purchaseqty_distr,            'dt_price_distr': dt_price_distr,            'dt_discount_distr': dt_discount_distr,            'dt_saleqty_distr': dt_saleqty_distr,            'dt_cost_distr': dt_cost_distr,            'dt_purchaseqty_distr': dt_purchaseqty_distr,            'aqi_price_distr': aqi_price_distr,            'aqi_discount_distr': aqi_discount_distr,            'aqi_saleqty_distr': aqi_saleqty_distr,            'aqi_cost_distr': aqi_cost_distr,            'aqi_purchaseqty_distr': aqi_purchaseqty_distr,            'dfx_tag': dfx_tag,            'pay_info': pay_info        }        df_train_val['storage_0'] = df_train_val['purchase_qty'] - df_train_val['sale_qty']        df_train_pre['storage_0'] = df_train_pre['purchase_qty'] - df_train_pre['sale_qty']        df_train_val['storage_1'] = df_train_val['storage_0'] + df_train_val['purchase_qty'] - df_train_val[            'sale_qty']        df_train_pre['storage_1'] = df_train_pre['storage_0'] + df_train_pre['purchase_qty'] - df_train_pre[            'sale_qty']        df_train_val['storage_2'] = df_train_val['storage_1'] + df_train_val['purchase_qty'] - df_train_val[            'sale_qty']        df_train_pre['storage_2'] = df_train_pre['storage_1'] + df_train_pre['purchase_qty'] - df_train_pre[            'sale_qty']        print(u"""库存的特征 ok""")        gy_storage0_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_0'].describe())        gy_storage0_distr.columns = ['storage0_' + col_nm for col_nm in gy_storage0_distr.columns]        gy_storage0_distr = gy_storage0_distr.reset_index().fillna(0.0)        gy_storage0_distr['storage0_static_meaning'] = gy_storage0_distr['storage0_count'].map(            lambda x: True if x > 35 else False)        gy_storage1_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_1'].describe())        gy_storage1_distr.columns = ['storage1_' + col_nm for col_nm in gy_storage1_distr.columns]        gy_storage1_distr = gy_storage1_distr.reset_index().fillna(0.0)        gy_storage1_distr['storage1_static_meaning'] = gy_storage1_distr['storage1_count'].map(            lambda x: True if x > 35 else False)        gy_storage2_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_2'].describe())        gy_storage2_distr.columns = ['storage2_' + col_nm for col_nm in gy_storage2_distr.columns]        gy_storage2_distr = gy_storage2_distr.reset_index().fillna(0.0)        gy_storage2_distr['storage2_static_meaning'] = gy_storage2_distr['storage2_count'].map(            lambda x: True if x > 35 else False)        df_train_pre = df_train_pre.drop(['storage_0', 'storage_1', 'storage_2'],                                         axis=1).reset_index().drop('index', axis=1)        print(u"验证集和训练集长度：", len(set(df_train_val.date)))        print(u"训练集和预测集长度：", len(set(df_train_pre.date)))        print(u"检查数据结构是否一致：", len(set(df_train_pre.date)) -              len(set(df_train_val.date)) - len(set(data_sql['actual_purchase_fcst'].date)))        storage_info = {'gy_storage0_distr': gy_storage0_distr,                        'gy_storage1_distr': gy_storage1_distr,                        'gy_storage2_distr': gy_storage2_distr                        }        return features_info, storage_info, df_train_pre_temp, df_train_valclass Submit_Result(object):    def __init__(self, sudoer, items_info_origin, dfx_tag, submission):        """        :param sudoer:        :param items_info_origin:        :param dfx_tag:        :param submission:        """        self.submission = submission        self.PRE_DAYS = sudoer.PRE_DAYS        self.items_info_origin = items_info_origin        self.dfx_tag = dfx_tag        self.NAME_STAMP = sudoer.NAME_STAMP        self.NAME_APP = sudoer.NAME_APP    def submission_result(self):        """        :return:        """        dfx = self.dfx_tag        submission = pd.DataFrame(self.submission)        submission.columns = ['date', 'storeid', 'goodsid', 'sale_qty', 'sale_int_qty']        submission['goodsid'] = submission['goodsid'].astype(str)        submission['storeid'] = submission['storeid'].astype(str)        dfx['goodsid'] = dfx['goodsid'].astype(str)        dfx['storeid'] = dfx['storeid'].astype(str)        submission = pd.merge(submission, dfx, on=['storeid', 'goodsid'], how='left')        submission["pred_sale_volume"] = np.clip(np.expm1(submission["sale_qty"]), 0, 10000000)        submission['pred_sale_volume'] = submission['pred_sale_volume']        submission.insert(0, 'time_create', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))        params = {}        params['Model_name'] = self.NAME_STAMP        params['Prediction_days'] = self.PRE_DAYS        params['MAX_ROUND'] = 2000        json_str = json.dumps(params)        submission.insert(1, 'model_param', params['Model_name'])        submission['goodsid'] = submission['goodsid'].astype(str)        submission_item = pd.merge(            self.items_info_origin.reset_index()[['goodsid', 'name', 'spname']].astype(str),            submission, on=['goodsid'], how='right')        submission_item['goodsname'] = submission_item['name'] + [':'] + submission_item['spname']        submission_df = submission_item[['time_create', 'date',                                         'storeid', 'goodsid',                                         'goodsname', 'pred_sale_volume',                                         'model_param']]        submission_df.to_csv(self.NAME_APP + 'submission_data.csv')        print(submission_df.head(10))        self.sub_to_mysql(submission_df)    def sub_to_mysql(self, submission_df):        """        :param submission_df:        :return:        """        database_name = 'CHAOSHIFA_DB'        table_name = 'model_predct_result_dev'        columns_nm = ['time_create', 'date', 'storeid', 'goodsid',                      'goodsname', 'pred_sale_volume', 'model_param']        submission_df.columns = columns_nm        self.DataFrame_to_MySQL(submission_df, database_name, table_name)    def DataFrame_to_MySQL(self, dataframe, database_name, table_name):        """        :param dataframe:        :param database_name:        :param table_name:        :return:        """        yconnect = create_engine('mysql+pymysql://sophon:IAmSophon_4523@47.97.17.51:3306/?charset=utf8')        pd.io.sql.to_sql(dataframe, table_name, yconnect,                         schema=database_name, if_exists='append', index=False)class Choose_Goods(object):    def __init__(self, sudoer, df_train_pre):        """        define the states of prediction        :param sudoer:        :param df_train_pre:        """        self.states = sudoer.states        self.data_train_pre = df_train_pre    def make_choice(self):        if self.states == 'all':            return self.data_train_pre        elif self.states == 'normal_24':            goods_list = [17927, 27720, 30051, 30052, 30053, 30055, 30061, 30062, 30065, 30069,                          30072, 30079, 30087, 30094, 30095, 30102, 30112, 30133, 30137, 30147, 30149, 30153,                          119230, 180159]            print('只预测24个常规品')            goods_df = pd.DataFrame(goods_list, columns=['goodsid'])            data_train_pre = pd.merge(self.data_train_pre, goods_df, on=['goodsid'], how='inner')            return data_train_pre        elif self.states == 'special_4':            goods_list = [30072, 30062, 30065, 30102]            print(u" 预测特殊的4个品类：奶油生菜 油菜，油麦菜，蒜苗")            goods_df = pd.DataFrame(goods_list, columns=['goodsid'])            data_train_pre = pd.merge(self.data_train_pre, goods_df, on=['goodsid'], how='inner')            return data_train_preclass Descriminater(object):    """    To make a Decision Matrix for different reasons:    :inside reasons: which means the reasons(features) which come from the good itself,i.e.,goodsid, sales,    sale quantities,price,cost,etc.    :outside reasons: Which means the reasons(features) which come from the outside environment,i.e.,temperature,    weather,AQI,store location,store profiles,etc.    """    def __init__(self, sudoer, features_done):        self.features_done = features_done        self.before_job_days = sudoer.before_job_days        self.before_weekends_days = sudoer.before_weekends_days        target_labels = ['sale_qty', 'price']  ## you can set it as ['sale_qty' ,'cost','purchase_qty', 'price' ]        All_Before_job_cols = []        All_Before_weekends_cols = []        for target_label in target_labels:            All_Before_job_cols += ['before_' + target_label + '_' + str(i).zfill(2) for i in                                    range(0, self.before_job_days)]            All_Before_weekends_cols += ['before_' + target_label + '_' + str(i).zfill(2) for i in                                         range(0, self.before_weekends_days)]        self.labels_inside_j = All_Before_job_cols        self.labels_inside_w = All_Before_weekends_cols        self.NAME_APP = sudoer.NAME_APP    def faiss_similarity_L2(self, feature_qarray, rows_qdata, k=10):        """        find the top k similarity for rows_qdata;        :param: feature_qarray:as query database. Dtype:dataframe        :param: rows_qdata: query entries.should less than 19. Dtype:dataframe        Faiss: A library for efficient similarity search and clustering of dense vectors.        https://github.com/facebookresearch/faiss.git        Setup for GPU and CPU:        https://github.com/facebookresearch/faiss/blob/master/INSTALL.md        if you have a GPU server,you can also setup it        """        Indexes = []        Distances = []        xb = np.array(feature_qarray).astype('float32')        xq = np.array(rows_qdata).astype('float32')        nq, d = xq.shape        print(u'相似比对数据的维度：', nq, d)        nb, d = xb.shape        print(u'需要搜索的数据维度：', nb, d)        xq = np.ascontiguousarray(xq)        xb = np.ascontiguousarray(xb)        index = faiss.IndexFlatL2(d)        # print(index.is_trained)        index.add(xb)        # print(index.ntotal)        for i in range(math.ceil(nq / 18)):            Distance, Index = index.search(xq[i * 18:(i + 1) * 18], k)            # print(Index)            # print(Distance)            Indexes += Index.tolist()            Distances += Distance.tolist()        return Distances, Indexes    def outside_decision(self, feature_job_qarray, feature_weekends_qarray, rows_job_qdata, rows_weekends_qdata):        """        This function goes to find the Decision Matrix which derived by outside reasons.        :return:        """        decision_feature_j = feature_job_qarray.drop(self.labels_inside_j, axis=1)        query_feature_j = rows_job_qdata.drop(self.labels_inside_j, axis=1)        Ds_j, Is_j = self.faiss_similarity_L2(decision_feature_j, query_feature_j, k=10)        decision_feature_w = feature_weekends_qarray.drop(self.labels_inside_w, axis=1)        query_feature_w = rows_weekends_qdata.drop(self.labels_inside_w, axis=1)        Ds_w, Is_w = self.faiss_similarity_L2(decision_feature_w, query_feature_w, k=10)        return Is_j, Is_w    def inside_decision(self, feature_job_qarray, feature_weekends_qarray, rows_job_qdata, rows_weekends_qdata):        """        This function goes to find the Decision Matrix which derived by inside reasons.        :return:        """        decision_feature_j = feature_job_qarray[self.labels_inside_j]        query_feature_j = rows_job_qdata[self.labels_inside_j]        Ds_j, Is_j = self.faiss_similarity_L2(decision_feature_j, query_feature_j, k=10)        decision_feature_w = feature_weekends_qarray[self.labels_inside_w]        query_feature_w = rows_weekends_qdata[self.labels_inside_w]        Ds_w, Is_w = self.faiss_similarity_L2(decision_feature_w, query_feature_w, k=10)        return Is_j, Is_w    def generate_decision_matrix(self, df_train_pre, data_rows):        """        After we finish seeking the best similar entires for NaNs, with this function we fill them to training        and prediction dataset, by the way we will update the feature infomationof prediction dataset.        :param:df_train_pre,original training and prediction dataset,Dtype,DataFrame.        :return: new_feture_done.Dtype,dict.        """        # A .TODO: fill 4 part blanks        # for past:        # 1. add the similarity sale_qty,        # 2. add price(better using mean value than NaN),        # 3. add cost(some times better using mean value than NaN),        # 4. add purchase(better using mean value than NaN)        # for future:        # 1. add price(better using mean value than NaN),        # 2. add cost(some times better using mean value than NaN),        # 3. add purchase(better using mean value than NaN)        rows_j_price_past = data_rows['rows_j_price_past']        rows_j_sale_past = data_rows['rows_j_sale_past']        rows_w_price_past = data_rows['rows_w_price_past']        rows_w_sale_past = data_rows['rows_w_sale_past']        rows_j_price_future = data_rows['rows_j_price_future']        rows_w_price_future = data_rows['rows_w_price_future']        rows_price_past = data_rows['rows_price_past']        rows_sale_past = data_rows['rows_sale_past']        rows_price_future = data_rows['rows_price_future']        feature_job_sp_qarray, feature_weekends_sp_qarray, rows_job_sp_qdata, rows_weekends_sp_qdata = self.data4faiss(            rows_sale_past)        insale_Is_jp, insale_Is_wp = self.inside_decision(feature_job_sp_qarray,                                                          feature_weekends_sp_qarray,                                                          rows_job_sp_qdata,                                                          rows_weekends_sp_qdata)        outsale_Is_jp, outsale_Is_wp = self.outside_decision(feature_job_sp_qarray,                                                             feature_weekends_sp_qarray,                                                             rows_job_sp_qdata,                                                             rows_weekends_sp_qdata)        feature_job_pp_qarray, feature_weekends_pp_qarray, rows_job_pp_qdata, rows_weekends_pp_qdata = self.data4faiss(            rows_price_past)        inprice_Is_jp, inprice_Is_wp = self.inside_decision(feature_job_pp_qarray,                                                            feature_weekends_pp_qarray,                                                            rows_job_pp_qdata,                                                            rows_weekends_pp_qdata)        outprice_Is_jp, outprice_Is_wp = self.outside_decision(feature_job_pp_qarray,                                                               feature_weekends_pp_qarray,                                                               rows_job_pp_qdata,                                                               rows_weekends_pp_qdata)        feature_job_pf_qarray, feature_weekends_pf_qarray, rows_job_pf_qdata, rows_weekends_pf_qdata = self.data4faiss(            rows_price_future)        inprice_Is_jf, inprice_Is_wf = self.inside_decision(feature_job_pf_qarray,                                                            feature_weekends_pf_qarray,                                                            rows_job_pf_qdata,                                                            rows_weekends_pf_qdata)        outprice_Is_jf, outprice_Is_wf = self.outside_decision(feature_job_pf_qarray,                                                               feature_weekends_pf_qarray,                                                               rows_job_pf_qdata,                                                               rows_weekends_pf_qdata)        df_train_pre_distri = df_train_pre.copy()        df_train_pre_distri = df_train_pre_distri.fillna(0.0)        df_train_pre_distri['sale_qty'] = df_train_pre_distri['sale_qty'].map(lambda x: x if x > 0 else np.nan)        df_train_pre_distri['price'] = df_train_pre_distri['price'].map(lambda x: x if x > 0 else np.nan)        df_train_pre_distri = df_train_pre_distri.dropna()        df_sale_distr = df_train_pre_distri.groupby(['storeid', 'goodsid']).apply(lambda x: x['sale_qty'].describe(        ))        df_sale_distr.columns = ['sale_qty_' + col_nm for col_nm in df_sale_distr.columns]        df_sale_distr = df_sale_distr.reset_index().fillna(0.0)        df_price_distr = df_train_pre_distri.groupby(['storeid', 'goodsid']).apply(lambda x: x['price'].describe(        ))        df_price_distr.columns = ['price_' + col_nm for col_nm in df_price_distr.columns]        df_price_distr = df_price_distr.reset_index().fillna(0.0)        df_price_sale_distr = pd.merge(df_sale_distr, df_price_distr, on=['storeid', 'goodsid'], how='left')        dm_sale_past_job = self.make_decision_matrix(df_price_sale_distr,                                                     'features_job',                                                     rows_j_sale_past,                                                     outsale_Is_jp,                                                     insale_Is_jp)        dm_sale_past_weekends = self.make_decision_matrix(df_price_sale_distr,                                                          'features_weekends',                                                          rows_w_sale_past,                                                          outsale_Is_wp,                                                          insale_Is_wp)        if len(dm_sale_past_job) > 0 and len(dm_sale_past_weekends) > 0:            dm_sale_past = pd.concat([dm_sale_past_job, dm_sale_past_weekends], axis=0).reset_index().drop(                'index', axis=1)        else:            if len(dm_sale_past_job) > 0:                dm_sale_past = dm_sale_past_job            elif len(dm_sale_past_weekends) > 0:                dm_sale_past = dm_sale_past_weekends        dm_price_past_job = self.make_decision_matrix(df_price_sale_distr,                                                      'features_job',                                                      rows_j_price_past,                                                      outprice_Is_jp,                                                      inprice_Is_jp)        dm_price_past_weekends = self.make_decision_matrix(df_price_sale_distr,                                                           'features_weekends',                                                           rows_w_price_past,                                                           outprice_Is_wp,                                                           inprice_Is_wp)        if len(dm_price_past_job) > 0 and len(dm_price_past_weekends) > 0:            dm_price_past = pd.concat([dm_price_past_job, dm_price_past_weekends], axis=0).reset_index().drop(                'index', axis=1)        else:            if len(dm_price_past_job) > 0:                dm_price_past = dm_price_past_job            elif len(dm_price_past_weekends) > 0:                dm_price_past = dm_price_past_weekends        dm_price_future_job = self.make_decision_matrix(df_price_sale_distr,                                                        'features_job',                                                        rows_j_price_future,                                                        outprice_Is_jf,                                                        inprice_Is_jf)        dm_price_future_weekends = self.make_decision_matrix(df_price_sale_distr,                                                             'features_weekends',                                                             rows_w_price_future,                                                             outprice_Is_wf,                                                             inprice_Is_wf)        if len(dm_price_future_job) > 0 and len(dm_price_future_weekends) > 0:            dm_price_future = pd.concat([dm_price_future_job, dm_price_future_weekends],                                        axis=0).reset_index().drop('index', axis=1)        else:            if len(dm_price_future_job) > 0:                dm_price_future = dm_price_future_job            elif len(dm_price_future_weekends) > 0:                dm_price_future = dm_price_future_weekends        rows_dm = {'dm_sale_past': dm_sale_past,                   'dm_price_past': dm_price_past,                   'dm_price_future': dm_price_future                   }        rows_dm['dm_sale_past'].to_csv(self.NAME_APP + 'dm_sale_past.csv')        rows_dm['dm_price_past'].to_csv(self.NAME_APP + 'dm_price_past.csv')        rows_dm['dm_price_future'].to_csv(self.NAME_APP + 'dm_price_future.csv')        return rows_dm    def make_decision_matrix(self, df_price_sale_distr, features_label, rows_target, outside_Is, inside_Is):        if len(rows_target) > 0:            ## outside            out_sale_indexs = []            out_price_indexs = []            for item in outside_Is:                out_sale_indexs += [self.features_done[features_label].loc[item]['sale_qty'].tolist()]                out_price_indexs += [self.features_done[features_label].loc[item]['price'].tolist()]            out_sale_indexs = pd.DataFrame(out_sale_indexs)            out_sale_indexs.columns = ['OutSaleTop_' + str(i).zfill(2) for i in range(0, 10)]            out_price_indexs = pd.DataFrame(out_price_indexs)            out_price_indexs.columns = ['OutPriceTop_' + str(i).zfill(2) for i in range(0, 10)]            ## inside            in_sale_indexs = []            in_price_indexs = []            for item in inside_Is:                in_sale_indexs += [self.features_done[features_label].loc[item]['sale_qty'].tolist()]                in_price_indexs += [self.features_done[features_label].loc[item]['price'].tolist()]            in_sale_indexs = pd.DataFrame(in_sale_indexs)            in_sale_indexs.columns = ['InSaleTop_' + str(i).zfill(2) for i in range(0, 10)]            in_price_indexs = pd.DataFrame(in_price_indexs)            in_price_indexs.columns = ['InPriceTop_' + str(i).zfill(2) for i in range(0, 10)]            rows_index_sale_price = pd.concat(                [rows_target, out_price_indexs, in_sale_indexs, in_price_indexs, out_sale_indexs], axis=1)            rows_index_sale_distr = pd.merge(rows_index_sale_price, df_price_sale_distr,                                             on=['storeid', 'goodsid'], how='left')        else:            rows_index_sale_distr = pd.DataFrame()        return rows_index_sale_distr        # B.TODO: use the new values for making time series        """        sale_qty_times_series  # no future info        cost_times_series        purchuse_time_series        price_time_series"""        # C.TODO:        # 1.Drop the before_labels features on old feature_done        # 2.add the New time_series above we had made.        # 3.return new feature_done  and make predictions.        # return new_feature_done    def data4faiss(self, rows):        """        """        # we limit the time of all days as short time term for study        feature_job_query_origin = self.features_done['features_job']        feature_job_query = feature_job_query_origin[~feature_job_query_origin.sale_qty.isnull()]        feature_job_query = feature_job_query[feature_job_query.sale_qty > 0.0]        feature_job_query = feature_job_query.reset_index().drop('index', axis=1)        feature_weekends_query_origin = self.features_done['features_weekends']        feature_weekends_query = feature_weekends_query_origin[~feature_weekends_query_origin.sale_qty.isnull()]        feature_weekends_query = feature_weekends_query[feature_weekends_query.sale_qty > 0.0]        feature_weekends_query = feature_weekends_query.reset_index().drop('index', axis=1)        # because those features do not include in the future information.        # we delete them for finding the most similar entries.        feature_job_qarray = feature_job_query.iloc[:, 4:].drop(['discount_degree',                                                                 'price',                                                                 'cost',                                                                 'campaign_nm_en',                                                                 'date_range_add',                                                                 'date_range',                                                                 'Poisson_distri'],                                                                axis=1).fillna(            0.0).reset_index().drop('index', axis=1)        feature_weekends_qarray = feature_weekends_query.iloc[:, 4:].drop(['discount_degree',                                                                           'price',                                                                           'cost',                                                                           'campaign_nm_en',                                                                           'date_range_add',                                                                           'date_range',                                                                           'Poisson_distri'],                                                                          axis=1).fillna(            0.0).reset_index().drop('index', axis=1)        rows['week'] = rows['date'].map(            lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))        print("""缺失 数据集划分两部分 按照工作日 和非工作日""")        df_0 = rows[rows['week'] == 0]        df_1 = rows[rows['week'] == 6]        rows_weekends = pd.concat([df_0, df_1], axis=0)        rows_job = rows[0 < rows['week']]        rows_job = rows_job[rows_job['week'] < 6]        rows_job = rows_job.drop('week', axis=1)        rows_weekends = rows_weekends.drop('week', axis=1)        rows_job_data = pd.merge(rows_job, feature_job_query_origin, on=['date', 'storeid', 'goodsid'],                                 how='left')        rows_weekends_data = pd.merge(rows_weekends, feature_weekends_query_origin,                                      on=['date', 'storeid', 'goodsid'], how='left')        rows_job_qdata = rows_job_data.iloc[:, 5:].drop(['discount_degree',                                                         'price',                                                         'cost',                                                         'campaign_nm_en',                                                         'date_range_add',                                                         'date_range',                                                         'Poisson_distri'], axis=1).fillna(            0.0).reset_index().drop('index', axis=1)        rows_weekends_qdata = rows_weekends_data.iloc[:, 5:].drop(['discount_degree',                                                                   'price',                                                                   'cost',                                                                   'campaign_nm_en',                                                                   'date_range_add',                                                                   'date_range',                                                                   'Poisson_distri'], axis=1).fillna(            0.0).reset_index().drop('index', axis=1)        return feature_job_qarray, feature_weekends_qarray, rows_job_qdata, rows_weekends_qdataclass Generater(object):    """    Generate the data that is NaN    """    def __init__(self, df_train_pre, rows_dm):        """        Generate the best suitable sale quantities for what we had found in Data2model.        :param NaN_found:        :param refer_data:        """        self.rows_dm = rows_dm        self.df_train_pre = df_train_pre    def outburst_sale_generater(self):        """        Generate the best sale for goods we concerted        :return:        """        dm_sale_past = self.rows_dm['dm_sale_past'].drop_duplicates(['date', 'storeid', 'goodsid'])        df_train_pre_needreset = pd.merge(dm_sale_past, self.df_train_pre, on=['storeid', 'date', 'goodsid'],                                          how='left')        data_mean = df_train_pre_needreset[['date',                                            'storeid',                                            'goodsid',                                            'sale_qty_mean',                                            'discount_degree',                                            'price_mean',                                            'cost',                                            'purchase_qty']]        data_mean.columns = ['date',                             'storeid',                             'goodsid',                             'sale_qty',                             'discount_degree',                             'price',                             'cost',                             'purchase_qty']        data_mean = data_mean.set_index(['date', 'storeid', 'goodsid'])        df_train_pre_temp = self.df_train_pre.set_index(['date', 'storeid', 'goodsid'])        df_train_pre_temp = df_train_pre_temp.reindex(            list(set(df_train_pre_temp.index) - set(data_mean))).append(data_mean)        df_train_pre_sale = df_train_pre_temp.reset_index()        df_train_pre_sale = df_train_pre_sale.drop_duplicates(['date', 'storeid', 'goodsid'])        return df_train_pre_sale    def future_price_generator(self, df_train_pre_sale):        dm_price_future = self.rows_dm['dm_price_future'].drop_duplicates(['date', 'storeid', 'goodsid'])        df_train_pre_needreset = pd.merge(dm_price_future, df_train_pre_sale, on=['storeid', 'date', 'goodsid'],                                          how='left')        data_mean = df_train_pre_needreset[['date',                                            'storeid',                                            'goodsid',                                            'sale_qty',                                            'discount_degree',                                            'price_mean',                                            'cost',                                            'purchase_qty']]        data_mean.columns = ['date',                             'storeid',                             'goodsid',                             'sale_qty',                             'discount_degree',                             'price',                             'cost',                             'purchase_qty']        data_mean = data_mean.set_index(['date', 'storeid', 'goodsid'])        df_train_pre_temp = df_train_pre_sale.set_index(['date', 'storeid', 'goodsid'])        df_train_pre_temp = df_train_pre_temp.reindex(            list(set(df_train_pre_temp.index) - set(data_mean))).append(data_mean)        df_train_pre_sale_price = df_train_pre_temp.reset_index()        df_train_pre_sale_price = df_train_pre_sale_price.drop_duplicates(['date', 'goodsid', 'storeid'])        return df_train_pre_sale_price    def Cold_Start_Sale_Generater(self):        """        To give a long term(at least as before days ) sales quanlity        :return:        """        cold_start_sales = []        return cold_start_sales    def Price_Generater(self):        """        Generate the best suitable price for goods we had found with NaN value in price list.        :return:        """        price = []        return priceclass Seek4nan(object):    """    :param: data,dataframe    数据预处理类：    针对天气数据、空气质量数据进行查询NaN位置和相应补全策略    补全策略采取：时间序列分析及预测。    辅助支持类：ARIMA    针对销量数据，冷启动，突发事件补齐。    """    def __init__(self, sudoer, features_done, limit_days=3):        """        instance initial.        :param input_data:dataframe type        :param target_label: list type.        """        self.features_done = features_done        self.limit_days = limit_days        self.TODAY_DATE = sudoer.TODAY_DATE        self.FT = sudoer.FT        self.NAME_APP = sudoer.NAME_APP        self.PRE_DAYS = sudoer.PRE_DAYS    def Driver4NaN_Seek(self, features_flag, target_label='sale_qty', states='future'):        """        """        if states == 'past':            limit_down_days = self.limit_days            limit_up_days = 0        elif states == 'future':            limit_down_days = 0            limit_up_days = self.PRE_DAYS            self.limit_days = self.PRE_DAYS        data_input = self.features_done[features_flag]        NaNs = []        # limit the date for one month: 30 days        data_input_limit = data_input[data_input.date >= self.FT(self.TODAY_DATE, -limit_down_days)]        data_input_limit = data_input_limit[data_input_limit.date < self.FT(self.TODAY_DATE, limit_up_days)]        data_input_limit_group = data_input_limit[['date',                                                   'storeid',                                                   'goodsid',                                                   target_label]].sort_values(by=['date']).groupby(            ['storeid', 'goodsid'])        dates_list = sorted(list(set(data_input_limit.date)))        num_flag = 0        for name, group in data_input_limit_group:            template = pd.DataFrame(dates_list)            template.columns = ['date']            template['storeid'] = name[0]            template['goodsid'] = name[1]            group_t = pd.merge(group, template, on=['date', 'storeid', 'goodsid'], how='outer').fillna(0.0)            group_t[target_label] = group_t[target_label].map(lambda x: np.nan if x == 0.0 else x)            group_t = group_t.sort_values(by=['date']).set_index('date')            row = self.find_nan(group_t, [target_label], states)            len_natural = len(group_t) - len(group)            print("{0} 这个品在 {1} 这家门店在 {7} 空缺 {2} 这个信息, 数据自然空缺 {3}次, {4}{5}天 空缺次数: {6}次。".format(name[1],                                                                                                name[0],                                                                                                target_label,                                                                                                len_natural,                                                                                                states,                                                                                                self.limit_days,                                                                                                len(row),                                                                                                features_flag.split(                                                                                                    '_')[                                                                                                    -1] + ' days'))            NaNs.append([name[1], name[0], len_natural, len(row)])            if num_flag == 0:                rows = row                num_flag = 1            else:                rows = rows.append(row)        NaNs = pd.DataFrame(NaNs)        if len(NaNs) > 0:            NaNs.columns = ['goodsid', 'storeid', '自然空缺数', str(self.limit_days) + '_日总空缺数']            NaNs.to_csv(                self.NAME_APP + 'NaNs_latest_' + target_label + '_' + str(self.limit_days) + '_days_records.csv')            # TODO sending email            rows = rows.reset_index().drop('index', axis=1)        else:            print('Congratulations,{0} ,and it is empty !'.format(                target_label + ' item did not find nan values in ' + features_flag.split('_')[-1] + ' in ' + str(                    self.limit_days) + ' ' + states + ' days'))            rows = pd.DataFrame()        return rows    def find_nan(self, input_data, target_label, states):        """        :param: data input        :output: return NaN values' index        """        def FTsub(date1, date2):            import datetime            date1 = time.strptime(date1, "%Y-%m-%d")            date2 = time.strptime(date2, "%Y-%m-%d")            date1 = datetime.datetime(date1[0], date1[1], date1[2], 0, 0, 0)            date2 = datetime.datetime(date2[0], date2[1], date2[2], 0, 0, 0)            return (date2 - date1).days        row_col = {}        for item_column in target_label:            # test data            goods_info = input_data[['storeid', 'goodsid']].values[0].tolist()            NaN_indexs = input_data[input_data[item_column].isnull()].index.tolist()            if len(NaN_indexs) > 2:                b1 = NaN_indexs[1:-1]                b2 = NaN_indexs[0:-2]                c = []                count = 1                flag = []                cut_index = []                cut_slice = []                for i in range(len(b1)):                    flag_signal = 1                    c.append(FTsub(b2[i], b1[i]))                    if FTsub(b2[i], b1[i]) == 1:                        count += 1                    else:                        flag_signal = 0                        flag.append(count)                        if flag_signal == 0:                            count += 1                flag.append(count)                for j in range(len(flag)):                    if j == 0:                        for item in NaN_indexs[0:flag[j]]:                            cut_slice.append([item, goods_info[0], goods_info[1]])                        cut_index.append(cut_slice)                        cut_slice = []                    elif j == len(flag) - 1:                        for item in NaN_indexs[flag[j - 1]:flag[j]]:                            cut_slice.append([item, goods_info[0], goods_info[1]])                        cut_slice.append([NaN_indexs[flag[j]], goods_info[0], goods_info[1]])                        cut_index.append(cut_slice)                        cut_slice = []                    else:                        for item in NaN_indexs[flag[j - 1]:flag[j]]:                            cut_slice.append([item, goods_info[0], goods_info[1]])                        cut_index.append(cut_slice)                        cut_slice = []                row_col[item_column] = cut_index            elif len(NaN_indexs) == 0:                # print('{0} columns does not exist NaN '.format(item_column))                row_col_index = []                row_col[item_column] = row_col_index            elif len(NaN_indexs) == 1:                row_col_index = [[NaN_indexs[0], goods_info[0], goods_info[1]]]                row_col[item_column] = [row_col_index]            elif len(NaN_indexs) == 2:                if FTsub(NaN_indexs[0], NaN_indexs[1]) == 1:                    row_col_index = [[NaN_indexs[0], goods_info[0], goods_info[1]],                                     [NaN_indexs[1], goods_info[0], goods_info[1]]]                else:                    row_col_index = [[NaN_indexs[0], goods_info[0], goods_info[1]]]                    row_col_index.append([NaN_indexs[1], goods_info[0], goods_info[1]])                row_col[item_column] = [row_col_index]        rows = []        # print(row_col)        for items in row_col[target_label[0]]:            if len(items) == 1:                rows.append(items[0] + ['Outburst_1'])            elif len(items) == 2:                for it in items:                    rows.append(it + ['Outburst_2'])            elif len(items) == 3:                for it in items:                    rows.append(it + ['Outburst_3'])            elif len(items) > 3 and len(items) <= 7:                for it in items:                    rows.append(it + ['Outburst_long'])            elif len(items) > 7 and len(items) < 30:                for it in items:                    rows.append(it + ['cold_start'])            elif len(items) >= 30:                for it in items:                    rows.append(it + ['not_on_saling'])            else:                rows = []        if len(rows) >= 1:            rows = pd.DataFrame(rows)            rows.columns = ['date', 'storeid', 'goodsid', target_label[0] + '_' + states + '_NaN_type']        else:            rows = pd.DataFrame()        return rowsdef main():    DATE = parse_arguments(sys.argv[1:]).DATE    # for test and develop    DATE = '2018-04-09'    print(DATE)    sudoer = Initialization(DATE)    ds = Get_Mysql_Data(sudoer)    dbinfo = Get_Mysql_Data(sudoer, 'COMMONINFO_DB')    data_sql = {        'actual_purchase_his': ds.get_df_actual_purchase_his(),        'actual_purchase_fcst': ds.get_df_actualpurchase_fcst(),        'airq_his': ds.get_df_aggrbydate_airq_his(),        'airq_fcst': ds.get_df_airq_fcst(),        'saleprice_his': ds.get_df_aggrbydate_saleprice(),        'weather_his': ds.get_df_aggrbydate_weather_his(),        'weather_fcst': ds.get_df_weather_fcst(),        'saleqty': ds.get_df_aggrbydate_saleqty(),        'category_his': ds.get_df_category_his(),        'goods_his': ds.get_df_goods_his(),        'canlendar_info': dbinfo.get_df_canlendar_info(),        'market_campaign_his': ds.get_market_campaign(),        'new_market_campaign_his': ds.df_get_campaign_his(),        'price_future': ds.get_price_future(),        'aggrbydate_pay_his': ds.get_mid_aggrbydate_num_of_pay_vege_data()    }    provider = Pre_Data(sudoer, data_sql)    features_info, storage_info, df_train_pre, df_train_val = provider.pre_features()    goods_selector = Choose_Goods(sudoer, df_train_pre)    df_train_pre = goods_selector.make_choice()    promotion = provider.pre_promotion()    train_data_provider = Get_Training_Data(sudoer, promotion, features_info, storage_info, df_train_pre)    features_done = train_data_provider.pre_features_for_separate_data()    Seeker = Seek4nan(sudoer, features_done, 60)    rows_j_price_past = Seeker.Driver4NaN_Seek('features_job', target_label='price', states='past')    rows_j_sale_past = Seeker.Driver4NaN_Seek('features_job', target_label='sale_qty', states='past')    rows_w_price_past = Seeker.Driver4NaN_Seek('features_weekends', target_label='price', states='past')    rows_w_sale_past = Seeker.Driver4NaN_Seek('features_weekends', target_label='sale_qty', states='past')    rows_j_price_future = Seeker.Driver4NaN_Seek('features_job', target_label='price', states='future')    rows_w_price_future = Seeker.Driver4NaN_Seek('features_weekends', target_label='price', states='future')    rows_price_past = pd.concat([rows_j_price_past, rows_w_price_past], axis=0).reset_index().drop('index',                                                                                                   axis=1)    rows_sale_past = pd.concat([rows_j_sale_past, rows_w_sale_past], axis=0).reset_index().drop('index',                                                                                                axis=1)    rows_price_future = pd.concat([rows_j_price_future, rows_w_price_future], axis=0).reset_index().drop(        'index', axis=1)    data_rows = {'rows_j_price_past': rows_j_price_past,                 'rows_j_sale_past': rows_j_sale_past,                 'rows_w_price_past': rows_w_price_past,                 'rows_w_sale_past': rows_w_sale_past,                 'rows_j_price_future': rows_j_price_future,                 'rows_w_price_future': rows_w_price_future,                 'rows_price_past': rows_price_past,                 'rows_sale_past': rows_sale_past,                 'rows_price_future': rows_price_future                 }    D = Descriminater(sudoer, features_done)    rows_dm = D.generate_decision_matrix(df_train_pre, data_rows)    G = Generater(df_train_pre, rows_dm)    df_train_pre_sale = G.outburst_sale_generater()    df_train_pre_sale_price = G.future_price_generator(df_train_pre_sale)    features_done['features_job'] = features_done['features_job'].drop(D.labels_inside_j, axis=1)    features_done['features_weekends'] = features_done['features_weekends'].drop(D.labels_inside_w, axis=1)    new_his_weekends, new_his_job = train_data_provider.make_data_two_part(df_train_pre_sale_price)    before_sale_w = sudoer.make_time_series(new_his_weekends, sudoer.before_weekends_days).set_index(['date',                                                                                                      'storeid',                                                                                                      'goodsid'])    before_sale_j = sudoer.make_time_series(new_his_job, sudoer.before_job_days).set_index(['date',                                                                                            'storeid',                                                                                            'goodsid'])    before_price_w = sudoer.make_time_series(new_his_weekends, sudoer.before_weekends_days, 'price').set_index(['date',                                                                                                                'storeid',                                                                                                                'goodsid'])    before_price_j = sudoer.make_time_series(new_his_job, sudoer.before_job_days, 'price').set_index(['date',                                                                                                      'storeid',                                                                                                      'goodsid'])    before_data_j = pd.concat([before_sale_j, before_price_j], axis=1).reset_index().set_index(['date',                                                                                                'storeid',                                                                                                'goodsid']).reset_index()    features_done['features_job'] = pd.merge(features_done['features_job'], before_data_j, on=['date', 'storeid',                                                                                               'goodsid'],                                             how='left').set_index(['date',                                                                    'storeid',                                                                    'goodsid',                                                                    'sale_qty'] + D.labels_inside_j).reset_index()    before_data_w = pd.concat([before_sale_w, before_price_w], axis=1).reset_index().set_index(['date',                                                                                                'storeid',                                                                                                'goodsid']).reset_index()    features_done['features_weekends'] = pd.merge(features_done['features_weekends'], before_data_w,                                                  on=['date', 'storeid',                                                      'goodsid'], how='left').set_index(['date',                                                                                         'storeid',                                                                                         'goodsid', 'sale_qty'                                                                                         ] + D.labels_inside_w).reset_index()    features_done['features_weekends'].to_csv('features_weekends_new.csv')    features_done['features_job'].to_csv('features_job_new.csv')    # TODO:deadline:submit to mysql:2018-05-23if __name__ == '__main__':    main()