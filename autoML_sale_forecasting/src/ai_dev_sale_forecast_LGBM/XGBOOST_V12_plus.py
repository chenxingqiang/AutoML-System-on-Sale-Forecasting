"""ThinkPage Retail Forecastingversion 3.0dev:xingqiang chendate:2018-04-11"""import gcimport sysimport timefrom math import expimport argparseimport datetime as DTimport jsonimport lightgbm as lgbimport numpy as npimport pandas as pdimport pymysqlimport xgboost as xgbfrom datetime import date, timedeltafrom sklearn import neighborsfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.externals import joblibfrom sklearn.metrics import mean_squared_errorfrom sklearn.preprocessing import LabelEncoderfrom sklearn.tree import ExtraTreeRegressorfrom sqlalchemy import create_enginedef parse_arguments(argv):    DATE = time.strftime('%Y-%m-%d', time.localtime(time.time()))    parser = argparse.ArgumentParser()    parser.add_argument('--DATE', type=str, help='the date of starting predction.', default=DATE)    return parser.parse_args(argv)class Inition_Values():    """    Define The Global Variables and Constants    """    def __init__(self,                 date,                 pre_days=3,                 data_nbrs=200,                 train_nbr=60,                 val_nbr=21,                 LTD_weekends=30,                 LTD_job=100,                 before_job_days=9,                 before_weekends_days=16,                 model_name='LightGBM',                 version='V12_plus1'):        """        :param date: today's date ,string type like '2018-03-14'        :param pre_days: in {1,2,3,..14}        :param data_nbrs:max_min=1335 min = train_nbr + max{round(before_job_days/5)*7,round(before_weekends_days/2)*7}        :param train_nbr: train_nbr at least more than 60        :param val_nbr: always about round({0.25~0.1}*train_nbr)        :param LTD_weekends: (LTD_weekends/2)*7 <= train_nbr        :param LTD_job: (LTD_job/5) *7  <= train_nbr        :param before_job_days: (data_nbrs-train_nbr) >=round(before_job_days/5)) *7        :param before_weekends_days: (data_nbrs-train_nbr) >=round(before_weekends_days/2)*7        :param model_name: XGBOOST,KNN,RFR,ETR,GBDT，LightGBM        """        self.TODAY_DATE = date        print(u"## 当前日期：" + self.TODAY_DATE)        self.PRE_DAYS = pre_days        print(u"## 预测天数:" + str(self.PRE_DAYS))        self.DATA_START_DAY = self.FT(self.TODAY_DATE, - data_nbrs)        print(u"## 获取数据的长度{}天,起点日期：".format(data_nbrs) + self.DATA_START_DAY)        self.TRAIN_START_DAY = self.FT(self.TODAY_DATE, -train_nbr)        print(u"##训练数据的长度:{}天,起点日期:".format(train_nbr) + self.TRAIN_START_DAY)        self.VAL_date = self.FT(self.TODAY_DATE, -val_nbr)        print(u"## 验证数据的起点日期：" + self.VAL_date)        self.PREDICTION_DAY = self.FT(self.TODAY_DATE, pre_days)        print(u"## 预测日期：" + self.PREDICTION_DAY)        self.TRAIN_DATE = self.date_dt(self.TRAIN_START_DAY)        self.VAL_DATE = self.date_dt(self.VAL_date)        self.PRE_DATE = self.date_dt(self.TODAY_DATE)        self.LTD_weekends = LTD_weekends        self.LTD_job = LTD_job        self.before_job_days = before_job_days        print(u'##  工作日的序列长度:', self.before_job_days)        self.before_weekends_days = before_weekends_days        print(u'##  休息日的序列长度:', self.before_weekends_days)        self.MODEL = model_name        print(u'##  预测的模型：', self.MODEL)        self.version = version        self.num_days = 7        self.NAME_APP = """{0}_{1}#{2}_{3}_{4}_{5}_{6}_{7}_{8}_{9}_{10}_""".format(model_name, version,                                                                                   before_weekends_days,                                                                                   LTD_weekends,                                                                                   before_job_days, LTD_job,                                                                                   val_nbr, train_nbr, data_nbrs,                                                                                   date, pre_days)    def T(self, FORMAT_DATE, i):        return (DT.datetime.strptime(FORMAT_DATE, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y%m%d')    def FT(self, FORMAT_DATE, i):        return (DT.datetime.strptime(FORMAT_DATE, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y-%m-%d')    def formatdate_a(self, d, i):        return (DT.datetime.strptime(d, '%Y-%m-%d') + DT.timedelta(i)).strftime('%Y-%m-%d')    def date_dt(self, DAY):        return date(int(DAY[0:4]), int(DAY[5:7]), int(DAY[8:]))class Get_mysql_data():    def __init__(self, sudoer, database='CHAOSHIFA_DB', host='47.97.17.51', user='sophon',                 password='IAmSophon_4523', port=3306):        print(database)        self.database = database        self.host = host        self.user = user        self.password = password        self.port = port        self.DATA_START_DAY = sudoer.DATA_START_DAY        self.TODAY_DATE = sudoer.TODAY_DATE    def parse(self, cols, sql):        """        :params:cols,list type        :params:sql_command,string type        :return: df,Dataframe type        """        print(cols, sql)        db = pymysql.connect(self.host, self.user, self.password, self.database, self.port, charset='utf8')        cursor = db.cursor()        cursor.execute(sql)        results = cursor.fetchall()        df = pd.DataFrame(list(results), columns=cols)        return df    def get_df_actual_purchase_his(self):        """        """        # purchase data from db        cols = ['date', 'storeid', 'goodsid', 'cost', 'purchase_qty']        sql_command = """                        SELECT                        date,storeid, goodsid,cost,qty                        FROM                        mid_aggrbydate_actualpurchase_his                        WHERE date >='{start_day}'                        AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE)        df_actual_purchase_his = self.parse(cols, sql)        return df_actual_purchase_his    def get_df_aggrbydate_airq_his(self):        """        """        # airq data from db        cols = ['date', 'storeid', 'AQI']        sql_command = """                        SELECT                         date,storeid, AQI                         FROM                          mid_aggrbydate_airq_his                         WHERE date >='{start_day}'                        AND date<'{today_date}'                        """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE)        df_aggrbydate_airq = self.parse(cols, sql)        return df_aggrbydate_airq    def get_df_aggrbydate_saleprice(self):        """        """        # price data from db        cols = ['date', 'storeid', 'goodsid', 'discount_degree', 'price']        sql_command = """                        SELECT                        date,storeid,goodsid,discount_degree,price                         FROM                        mid_aggrbydate_saleprice_his                         WHERE date >='{start_day}'                        AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE)        df_aggrbydate_saleprice = self.parse(cols, sql)        return df_aggrbydate_saleprice    def get_df_aggrbydate_saleqty(self):        """        """        # price data from db        cols = ['date', 'storeid', 'goodsid', 'sale_qty']        sql_command = """                        SELECT                         date,storeid,goodsid,sale_qty                         FROM                        mid_aggrbydate_saleqty_his                         WHERE date >='{start_day}'                         AND date<'{today_date}'                    """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE)        df_aggrbydate_saleqty = self.parse(cols, sql)        return df_aggrbydate_saleqty    def get_df_aggrbydate_weather_his(self):        """        """        # price data from db        cols = ['date', 'storeid', 'tem', 'windspeed', 'pre1h', 'rhu']        sql_command = """                        SELECT                         date,storeid,tem,windspeed,pre1h,rhu                         FROM                        mid_aggrbydate_weather_his                         WHERE date >='{start_day}'                        AND date<'{today_date}'                        """        sql = sql_command.format(start_day=self.DATA_START_DAY, today_date=self.TODAY_DATE)        df_aggrbydate_weather_his = self.parse(cols, sql)        return df_aggrbydate_weather_his    def get_df_category_his(self):        """        """        # category data from db        cols = ['deptid', 'name', 'name_up1', 'name_up2',                'name_up3', 'name_up4', 'deptlevelid', 'date']        sql_command = """                        SELECT                         deptid,name,name_up1,name_up2,                        name_up3,name_up4, deptlevelid, date                         FROM                        raw_cleaned_category_his                         WHERE date ='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df_category_his = self.parse(cols, sql)        return df_category_his    def get_df_goods_his(self):        """        """        # category data from db        cols = ['goodsid', 'spname', 'ppname', 'deptid',                'goodstypeid', 'stocktype', 'saletaxrate',                'spec', 'unitname', 'origin', 'keepdays',                'indate', 'date']        sql_command = """                        SELECT                         goodsid, spname,ppname,deptid,goodstypeid,stocktype,                        saletaxrate,spec,unitname,origin,keepdays,indate,date                         FROM                          raw_cleaned_goods_his                         WHERE date ='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df_goods_his = self.parse(cols, sql)        return df_goods_his    def get_df_canlendar_info(self):        """        """        # category data from db        cols = ['date', 'weekday', '24term_name_en',                '24term_code', '24term_name_cn', 'holiday_bylaw']        sql_command = """                        SELECT                         date, weekday,24term_name_en,                        24term_code,24term_name_cn,holiday_bylaw                         FROM                         calendar                         WHERE date >='{start_day}'                         """        sql = sql_command.format(start_day=self.DATA_START_DAY)        df_canlendar_info = self.parse(cols, sql)        return df_canlendar_info    def get_df_actualpurchase_fcst(self):        """        """        cols = ['date', 'goodsid', 'cost', 'storeid']        sql_command = """                        SELECT                        date,goodsid,cost,storeid                         FROM                        mid_aggrbydate_actualpurchase_fcst                        WHERE date>='{today_date}'                        AND date_cur='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df = self.parse(cols, sql)        return df    def get_df_weather_fcst(self):        """        """        cols = ['date', 'storeid', 'tem', 'windspeed', 'pre1h', 'rhu']        sql_command = """                        SELECT                         date,storeid,tem,windspeed,pre1h,rhu                        FROM                         mid_aggrbydate_weather_fcst                        WHERE date>='{today_date}'                         AND date_cur='{today_date}'                    """        sql = sql_command.format(today_date=self.TODAY_DATE)        df = self.parse(cols, sql)        return df    def get_df_airq_fcst(self):        """        """        cols = ['date', 'date_time', 'city', 'AQI']        sql_command = """                        SELECT date,date_time,city, aqi                         FROM                         raw_cleaned_airq_fcst                         WHERE date >='{today_date}'                        """        sql = sql_command.format(today_date=self.TODAY_DATE)        df = self.parse(cols, sql)        df = pd.DataFrame(df.groupby(['date', 'city']).AQI.max()).reset_index().drop(['city'], axis=1)        df.insert(1, 'storeid', 'A035')        return df    def get_market_campaign(self):        """        """        # price data from db        cols = ['goodsid', 'storeid', 'daterange', 'campaign_type_cn', 'campaign_type_en']        sql = """                SELECT                 goodsid,storeid,date_range,campaign_type_en,campaign_type_en                FROM                  market_campaign             """        df_x = self.parse(cols, sql)        def data_deaf(x, z, l):            m = l.split('_')[0]            n = l.split('_')[1]            a = [str(a).split(' ')[0] for a in pd.date_range(m, n)]            y = pd.DataFrame(a, columns=['date'])            y['goodsid'] = x            y['campaign_type_en'] = z            return y        flag = 0        for item in df_x.values:            if len(item) == 0:                print('XXXX')                break            else:                x = data_deaf(item[0], item[4], item[2])                if flag == 0:                    data_temp = x                    flag = 1                else:                    data_temp = pd.concat([data_temp, x], axis=0)        df = data_temp.reset_index().drop('index', axis=1)        le = LabelEncoder()        for col_name in ['campaign_type_en']:            df[col_name] = le.fit_transform(df[col_name].values)        return df    def df_get_campaign_his(self):        """        """        cols = ['date', 'storeid', 'goodsid', 'campaign_nm_en', 'date_range', 'effect_est']        sql = """                SELECT                date,storeid,goodsid,campaign_nm_en,date_range,effect_est                FROM                mid_aggrbydate_campaign_forfeature                """        df = self.parse(cols, sql)        return dfclass Get_Training_Data():    """    """    def __init__(self, sudoer, promotion, features_info, storage_info, df_train_pre, target_name='sale_qty'):        self.df_train_pre = df_train_pre        self.condition = features_info['condition']        self.items_info = features_info['items_info']        self.before_weekends_days = sudoer.before_weekends_days        self.before_job_days = sudoer.before_job_days        self.TRAIN_START_DAY = sudoer.TRAIN_START_DAY        self.promotion_info = promotion['data_market_campin']        self.goods_statistac = pd.concat([features_info['gy_price_distr'].set_index(['goodsid', 'day_type']),                                          features_info['gy_discount_distr'].set_index(                                              ['goodsid', 'day_type']),                                          features_info['gy_saleqty_distr'].set_index(                                              ['goodsid', 'day_type']),                                          features_info['gy_cost_distr'].set_index(['goodsid', 'day_type']),                                          features_info['gy_purchaseqty_distr'].set_index(                                              ['goodsid', 'day_type'])                                          ], axis=1).reset_index()        self.storage_info = pd.concat([storage_info['gy_storage0_distr'].set_index('goodsid'),                                       storage_info['gy_storage1_distr'].set_index('goodsid'),                                       storage_info['gy_storage2_distr'].set_index('goodsid')                                       ], axis=1).reset_index()        self.tags_of_volume = ['date', 'storeid', 'goodsid', target_name]        self.tags_labels = ['date', 'storeid', 'goodsid']        self.tags_ts = ['storeid', 'goodsid', target_name]        self.tags_tiny = ['storeid', 'goodsid']        self.target_name = target_name    def pre_features_for_separate_data(self):        history_data = self.df_train_pre        history_data = history_data[history_data.date >= self.TRAIN_START_DAY]        print("""结合促销信息：促销效果用 Posion 分布表示""")        history_data = pd.merge(history_data, self.promotion_info,                                on=['date', 'goodsid', 'storeid'], how='left')        history_data['onpromotion'] = history_data['discount_degree'].map(            lambda x: 1 if x > 0 else 0).astype(bool)        print("""LR 函数归一化 销量""")        history_data['sale_qty'] = history_data['sale_qty'].map(            lambda x: np.log1p(float(x)) if float(x) > 0 else 0)        print("""结合 时间信息的特征：年 月 日 周几""")        history_data['DAY'] = history_data['date'].map(            lambda x: x.split('-')[-1]).astype(int)        history_data['MONTH'] = history_data['date'].map(            lambda x: x.split('-')[1]).astype(int)        history_data['YEAR'] = history_data['date'].map(            lambda x: x.split('-')[0]).astype(int)        history_data['week'] = history_data['date'].map(            lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))        print("""数据集划分两部分 按照工作日 和非工作日""")        df_0 = history_data[history_data['week'] == 0]        df_1 = history_data[history_data['week'] == 6]        history_weekends = pd.concat([df_0, df_1], axis=0)        history_job = history_data[0 < history_data['week']]        history_job = history_job[history_job['week'] < 6]        history_weekends.sort_values(['date'], inplace=True, ascending=True)        history_job.sort_values(['date'], inplace=True, ascending=True)        ### make training and prediction dataset        week_one_hot_j = pd.get_dummies(history_job.week, 'week')        ### make training and prediction dataset        week_one_hot_w = pd.get_dummies(history_weekends.week, 'week')        features_w = pd.concat([history_weekends, week_one_hot_w], axis=1)        features_w = pd.merge(features_w, self.condition, on=['storeid', 'date'], how='left')        features_w = pd.merge(features_w, self.items_info.reset_index(), on=['goodsid'], how='left')        features_w = pd.merge(features_w, self.goods_statistac, on=['goodsid', 'day_type'], how='left')        features_w = pd.merge(features_w, self.storage_info, on=['goodsid'], how='left')        features_w = features_w.drop(features_w[['sale_qty']], axis=1)        features_j = pd.concat([history_job, week_one_hot_j], axis=1)        features_j = pd.merge(features_j, self.condition, on=['storeid', 'date'], how='left')        features_j = pd.merge(features_j, self.items_info.reset_index(), on=['goodsid'], how='left')        features_j = pd.merge(features_j, self.goods_statistac, on=['goodsid', 'day_type'], how='left')        features_j = pd.merge(features_j, self.storage_info, on=['goodsid'], how='left')        features_j = features_j.drop(features_j[['sale_qty']], axis=1)        before_data_w = self.make_time_series(history_weekends, self.before_weekends_days)        before_data_j = self.make_time_series(history_job, self.before_job_days)        before_cost_w = self.make_time_series(history_weekends, self.before_weekends_days, 'cost')        before_cost_j = self.make_time_series(history_job, self.before_job_days, 'cost')        before_price_w = self.make_time_series(history_weekends, self.before_weekends_days, 'price')        before_price_j = self.make_time_series(history_job, self.before_job_days, 'price')        features_w = pd.merge(before_cost_w, features_w, how='left',                              on=['date', 'storeid', 'goodsid', 'cost'])        features_j = pd.merge(before_cost_j, features_j, how='left',                              on=['date', 'storeid', 'goodsid', 'cost'])        features_w = pd.merge(before_price_w, features_w, how='left',                              on=['date', 'storeid', 'goodsid', 'price'])        features_j = pd.merge(before_price_j, features_j, how='left',                              on=['date', 'storeid', 'goodsid', 'price'])        features_end_weekends = pd.merge(before_data_w, features_w, how='left',                                         on=['date', 'storeid', 'goodsid'])        features_end_job = pd.merge(before_data_j, features_j, how='left', on=['date', 'storeid',                                                                               'goodsid'])        features_end_job.to_csv('feature_end_job.csv')        features_end_weekends.to_csv('features_end_weekends.csv')        return {            'features_weekends': features_end_weekends,            'features_job': features_end_job        }    ## make the time series    def make_time_series(self, data, before_days, target_label='sale_qty'):        """        """        print(u"""制作 """ + target_label + u""" 时间序列....中""")        tags = self.tags_labels + [target_label]        tags_ts = self.tags_tiny + [target_label]        dates = sorted(list(set(data.date)), reverse=True)        df_all_day = data[tags]        df_day_0 = df_all_day[df_all_day.date == dates[1]]        df_day_1 = df_all_day[df_all_day.date == dates[2]]        df_day_0 = df_day_0[tags_ts]        df_day_1 = df_day_1[tags_ts]        df_day_0 = df_day_0.rename(columns={target_label: dates[1]})        df_day_1 = df_day_1.rename(columns={target_label: dates[2]})        df_day_n = pd.merge(df_day_0, df_day_1, how='outer', on=['storeid', 'goodsid'])        for day in dates[3:]:            df_day = df_all_day[df_all_day.date == day]            df_day_nx = df_day[tags_ts]            df_day_nx = df_day_nx.rename(columns={target_label: day})            df_day_n = pd.merge(df_day_n, df_day_nx, how='outer', on=['storeid', 'goodsid'])        df_day_n = df_day_n.fillna(0)        if self.before_job_days > self.before_weekends_days:            Before_cols = ['before_' + target_label + '_' + str(i).zfill(2) for i in range(0, self.before_job_days)]        else:            Before_cols = ['before_' + target_label + '_' + str(i).zfill(2) for i in range(0,                                                                                           self.before_weekends_days)]        before_0 = df_day_n[df_day_n.columns[-before_days:]]        before_0.columns = Before_cols[0:before_days]        before_title = df_day_n[self.tags_tiny]        before_end_0 = pd.concat([before_title, before_0], axis=1)        before_end_0['date'] = df_day_n.columns[-(before_days + 1)]        historyday_0 = data[data.date.isin([df_day_n.columns[-(before_days + 1)]])]        historyday_0 = historyday_0[tags]        df_change = before_end_0['date']        before_end_0 = before_end_0.drop('date', axis=1)        before_end_0.insert(2, 'date', df_change)        before_end_0 = pd.merge(historyday_0, before_end_0, how='left',                                on=self.tags_labels)        before_end_0 = before_end_0.drop_duplicates()        before_1 = df_day_n[df_day_n.columns[-(before_days + 1):-1]]        before_1.columns = Before_cols[0:before_days]        before_title = df_day_n[self.tags_tiny]        before_end_1 = pd.concat([before_title, before_1], axis=1)        before_end_1['date'] = df_day_n.columns[-(before_days + 2)]        historyday_1 = data[data.date.isin([df_day_n.columns[-(before_days + 2)]])]        historyday_1 = historyday_1[tags]        df_change = before_end_1['date']        before_end_1 = before_end_1.drop('date', axis=1)        before_end_1.insert(2, 'date', df_change)        before_end_1 = pd.merge(historyday_1, before_end_1, how='left',                                on=self.tags_labels)        ## drop duplicartes        before_end_1 = before_end_1.drop_duplicates()        before_data = pd.concat([before_end_0, before_end_1], axis=0)        for i in range(0, len(dates[1:-(before_days + 2)]) + 1):            before = df_day_n[df_day_n.columns[-(before_days + 2) - i:-2 - i]]            before.columns = Before_cols[0:before_days]            before_title = df_day_n[self.tags_tiny]            before_end = pd.concat([before_title, before], axis=1)            before_end['date'] = dates[-(before_days + 2) - i - 1]            historyday = data[data.date.isin([dates[-(before_days + 2) - i - 1]])]            historyday = historyday[tags]            df_change = before_end['date']            before_end = before_end.drop('date', axis=1)            before_end.insert(2, 'date', df_change)            before_end = pd.merge(historyday, before_end, how='left',                                  on=self.tags_labels)            ## drop duplicartes            before_end = before_end.drop_duplicates()            before_data = pd.concat([before_data, before_end], axis=0)        return before_dataclass Make_timeseries_features(object):    """    """    def __init__(self, sudoer, df_train_pre, items_info):        """        initialize  of the  class        :param sudoer: father nodes        :param df_train_pre: the dataset of train and pre        :param items_info: goods info        """        df_train_pre['date'] = pd.to_datetime(df_train_pre['date'])        ## 对销量值进行转换        df_train_pre['sale_qty'] = df_train_pre['sale_qty'].map(lambda x: np.log1p(            float(x)) if float(x) > 0 else 0)        df_train_pre['on_promotion'] = df_train_pre['discount_degree'].map(lambda x: 1 if x > 0 else 0).astype(            bool)        self.df_train_pre = df_train_pre        self.TODAY_DATE = sudoer.TODAY_DATE        self.TRAIN_DATE = sudoer.TRAIN_DATE        self.PRE_DATE = sudoer.PRE_DATE        self.VAL_DATE = sudoer.VAL_DATE        self.PREDICTION_DAY = sudoer.PREDICTION_DAY        self.PRE_DAYS = sudoer.PRE_DAYS        self.DATA_START_DAY = sudoer.DATA_START_DAY        self.num_days = sudoer.num_days        self.TRAIN_START_DAY = sudoer.TRAIN_START_DAY        self.date_dt = sudoer.date_dt        self.items_info = items_info        self.FT = sudoer.FT    def chart_data(self):        """        :param self        :return: dataset for train and predict        """        df_train_test = self.df_train_pre        ## 商品信息        item_info = self.items_info        le = LabelEncoder()        ##门店特征的汇总        store = pd.DataFrame()        store['storeid'] = ['A035', 'A206']        store['city'] = ['Beijing', 'Beijing']        store['state'] = ['open', 'open']        store['type'] = ['Middle', 'small']        store = store.set_index("storeid")        store['city'] = le.fit_transform(store['city'].values)        store['state'] = le.fit_transform(store['state'].values)        store['type'] = le.fit_transform(store['type'].values)        stores_info = store        for step in range(len(pd.date_range(self.TRAIN_START_DAY, self.TODAY_DATE))):            print(step)            df_train = df_train_test[df_train_test.date < self.FT(self.TODAY_DATE, -step)]  # validate_date            df_test_temp = df_train_test[df_train_test.date >= self.FT(self.TODAY_DATE, -step)]  ## validate_date            df_test_temp = df_test_temp[df_test_temp.date <= self.FT(self.PREDICTION_DAY, -step)]  ##            # validate_date+3 = pre_date            df_test = df_test_temp.drop(['sale_qty'], axis=1).dropna().drop_duplicates().reset_index()            df_test = df_test.set_index(['storeid', 'goodsid', 'date'])            df_test = df_test.drop_duplicates()            df_time = df_train.loc[df_train.date >= self.DATA_START_DAY]  ##获取数据的起点：start_date            promo_time = self.make_time_series(df_time, df_test, ["storeid", "goodsid", "date"], "on_promotion")            price_time = self.make_time_series(df_time, df_test, ["storeid", "goodsid", "date"], "price")            discount_time = self.make_time_series(df_time, df_test, ["storeid", "goodsid", "date"],                                                  "discount_degree")            df_time = df_time.set_index(                ["storeid", "goodsid", "date"])[["sale_qty"]].unstack(                level=-1).fillna(0)            df_time.columns = df_time.columns.get_level_values(1)            items_info = item_info.reindex(df_time.index.get_level_values(1))            items_info["perishable"] = items_info["keepdays"].map(lambda x: 1 if x < 3 else 0)            stores = stores_info.reindex(df_time.index.get_level_values(0))            df_time_item = df_time.groupby('goodsid')[df_time.columns].sum()            promo_time_item = promo_time.groupby('goodsid')[promo_time.columns].sum()            price_time_item = price_time.groupby('goodsid')[price_time.columns].sum()            discount_time_item = discount_time.groupby('goodsid')[discount_time.columns].sum()            df_time_store_class = df_time.reset_index()            df_time_store_class['keepdays'] = items_info['keepdays'].values            df_time_store_class_index = df_time_store_class[['keepdays', 'storeid']]            df_time_store_class = df_time_store_class.groupby(['keepdays', 'storeid'])[df_time.columns].sum()            df_time_promo_store_class = promo_time.reset_index()            df_time_promo_store_class['keepdays'] = items_info['keepdays'].values            df_time_promo_store_class_index = df_time_promo_store_class[['keepdays', 'storeid']]            df_time_promo_store_class = df_time_promo_store_class.groupby(['keepdays', 'storeid'])[                promo_time.columns].sum()            ##TODO 统计每天订单量            print(u"Preparing dataset...")            PRE_DATE_step = self.date_dt(self.FT(self.TODAY_DATE, -step))            X_test = self.prepare_dataset(df_time, 'promotion_x', promo_time, PRE_DATE_step,                                          is_train=False)            X_test2 = self.prepare_dataset(df_time_item, 'promotion', promo_time_item, PRE_DATE_step, is_train=False,                                           name_prefix='promo_item')            X_test2.index = df_time_item.index            X_test2 = X_test2.reindex(df_time.index.get_level_values(1)).reset_index(drop=True)            X_test3 = self.prepare_dataset(df_time_item, 'price', price_time_item, PRE_DATE_step, is_train=False,                                           name_prefix='price_item')            X_test3.index = df_time_item.index            X_test3 = X_test3.reindex(df_time.index.get_level_values(1)).reset_index(drop=True)            X_test4 = self.prepare_dataset(df_time_item, 'discount', discount_time_item, PRE_DATE_step,                                           is_train=False,                                           name_prefix='discount_item')            X_test4.index = df_time_item.index            X_test4 = X_test4.reindex(df_time.index.get_level_values(1)).reset_index(drop=True)            X_test5 = self.prepare_dataset(df_time_store_class, 'keepdays_store_class', df_time_promo_store_class,                                           PRE_DATE_step,                                           is_train=False, name_prefix='store_class')            X_test5.index = df_time_store_class.index            X_test5 = X_test5.reindex(df_time_store_class_index).reset_index(drop=True)            X_test = pd.concat(                [X_test, X_test2, X_test3, X_test4, X_test5,                 stores.reset_index()['storeid'], items_info.reset_index()['goodsid']], axis=1)            del X_test2, X_test3, X_test4, X_test5, df_time_promo_store_class, df_time_store_class_index            del df_time_item, promo_time_item, df_time_store_class, df_test, df_time, stores, items_info            gc.collect()            X_test['date'] = self.FT(self.TODAY_DATE, -step)            if step == 0:                X_test = X_test            else:                X_test = X_test.append(X_test)        datasets = {"X_test": X_test                    }        return datasets    def make_time_series(self, data_df, data_test, index_name, feature_name):        """        输入：特征集合:data_df，DataFrame;测试部分的特征：data_test，DataFrame             索引名称：list type；特征名称：字符串        输出：时间序列化后的特征，DataFrame        """        temp_train = data_df.set_index(index_name)[[feature_name]].unstack(            level=-1).fillna(0.0)        temp_train.columns = temp_train.columns.get_level_values(1)        temp_test = data_test[[feature_name]].unstack(level=-1).fillna(0.0)        temp_test.columns = temp_test.columns.get_level_values(1)        temp_test = temp_test.reindex(temp_train.index).fillna(0.0)        temp = pd.concat([temp_train, temp_test], axis=1)        del temp_test, temp_train        return temp    def get_timespan(self, df, dt, minus, periods, freq='D'):        return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]    def prepare_dataset(self, df, target_name, target_df, ttime, is_train=True, name_prefix=None):        X = {            target_name + "_20_time": self.get_timespan(target_df, ttime, 20, 20).sum(axis=1).values,            target_name + "_30_time": self.get_timespan(target_df, ttime, 30, 30).sum(axis=1).values,            target_name + "_60_time": self.get_timespan(target_df, ttime, 60, 60).sum(axis=1).values,        }        # print(X)        for i in [1, 2, 3, 5, 6, 7]:            tmp1 = self.get_timespan(df, ttime, i, i)            tmp2 = (self.get_timespan(target_df, ttime, i, i) > 0) * 1            X['has_' + target_name + '_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values            X['has_' + target_name + '_mean_%s_decay' % i] = (                    tmp1 * tmp2.replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(                axis=1).values            X['no_' + target_name + '_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(                axis=1).values            X['no_' + target_name + '_mean_%s_decay' % i] = (                    tmp1 * (1 - tmp2).replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(                axis=1).values        for i in [1, 2, 3, 5, 7]:            tmp = self.get_timespan(df, ttime, i, i)            X['diff_%s_mean' % i] = tmp.diff(axis=1).mean(axis=1).values            X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values            X['mean_%s' % i] = tmp.mean(axis=1).values            X['median_%s' % i] = tmp.median(axis=1).values            X['min_%s' % i] = tmp.min(axis=1).values            X['max_%s' % i] = tmp.max(axis=1).values            X['std_%s' % i] = tmp.std(axis=1).values        for i in [1, 2, 3, 5, 7]:            tmp = self.get_timespan(df, ttime + timedelta(days=-7), i, i)            X['diff_%s_mean_2' % i] = tmp.diff(axis=1).mean(axis=1).values            X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values            X['mean_%s_2' % i] = tmp.mean(axis=1).values            X['median_%s_2' % i] = tmp.median(axis=1).values            X['min_%s_2' % i] = tmp.min(axis=1).values            X['max_%s_2' % i] = tmp.max(axis=1).values            X['std_%s_2' % i] = tmp.std(axis=1).values        for i in [1, 2, 3, 4, 5, 6, 7]:            tmp = self.get_timespan(df, ttime, i, i)            X['has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values            X['last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values            X['first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values            tmp = self.get_timespan(target_df, ttime, i, i)            X['has_' + target_name + '_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values            X['last_has_' + target_name + '_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(                axis=1).values            X['first_has_' + target_name + '_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(                axis=1).values        for i in range(1, self.PRE_DAYS):            X['day_%s_time' % i] = self.get_timespan(df, ttime, i, 1).values.ravel()        for i in range(7):            X['mean_4_dow{}_time'.format(i)] = self.get_timespan(df, ttime, 28 - i, 4, freq='7D').mean(                axis=1).values            X['mean_20_dow{}_time'.format(i)] = self.get_timespan(df, ttime, 70 - i, 10, freq='7D').mean(                axis=1).values        X = pd.DataFrame(X)        if is_train:            y = df[                pd.date_range(ttime, periods=self.PRE_DAYS)            ].values            return X, y        if name_prefix is not None:            X.columns = ['%s_%s' % (name_prefix, c) for c in X.columns]        return Xclass Pre_data():    def __init__(self, sudoer, data_sql):        self.data_sql = data_sql        self.FT = sudoer.FT        self.TODAY_DATE = sudoer.TODAY_DATE    def pre_promotion(self):        def myfact(n):            """            """            n = int(n)            if n > 20:                n = 20            if n < 2:                return 1            else:                return n * myfact(n - 1)        flag = 0        gy_market_campaign = self.data_sql['new_market_campaign_his'].groupby(            ['storeid', 'goodsid', 'date_range'])        for name, group in gy_market_campaign:            if len(group) <= 7:                date_length = int(len(group) * 3 / 2)            else:                date_length = len(group)            m = group.effect_est.min()            start_date = group.date.values[0]            dates_add = pd.DataFrame([self.FT(start_date, i) for i in range(date_length)], columns=['date'])            dates_add = dates_add.reset_index()            dates_add['storeid'] = 'A035'            dates_add['goodsid'] = group.goodsid.values[0]            dates_add['campaign_nm_en'] = group.campaign_nm_en.values[0]            dates_add['date_range_add'] = start_date + '_' + self.FT(start_date, date_length)            dates_add['date_range'] = group.date_range.values[0]            dates_add['idx_m'] = dates_add['index'].map(lambda x: m ** x)            dates_add['idx_E'] = dates_add['index'].apply(myfact)            ## 泊松分布            dates_add['Poisson_distri'] = dates_add['idx_m'] / dates_add['idx_E']            dates_add['Poisson_distri'] = dates_add['Poisson_distri'].map(lambda x: x * exp(-m))            if flag == 0:                data_app = dates_add                flag = 1            else:                data_app = data_app.append(dates_add)        data_market_campin = data_app.reset_index().drop(['index', 'idx_m', 'idx_E', 'date_range_add'], axis=1)        le = LabelEncoder()        for col_name in ['campaign_nm_en', 'date_range']:            if data_market_campin[[col_name]].dtypes[0] == 'object':                data_market_campin[col_name] = le.fit_transform(data_market_campin[col_name].values)        promotion = {            'data_market_campin': data_market_campin        }        return promotion    def pre_features(self):        data_sql = self.data_sql        airq = data_sql['airq_his'].append(data_sql['airq_fcst']).set_index(['storeid', 'date'])        weather = data_sql['weather_his'].append(data_sql['weather_fcst']).set_index(['storeid', 'date'])        canlendar_info = data_sql['canlendar_info']        airq_weather = pd.concat([airq, weather], axis=1).fillna(0.0).reset_index()        airq_weather_canlendar = pd.merge(airq_weather, canlendar_info, how='left', on=['date']).fillna(            'Ordinary_day').drop(            ['24term_name_en', '24term_code'], axis=1)        airq_weather_canlendar['day_type'] = airq_weather_canlendar['weekday'].map(            lambda x: 'work_day' if 1 <= x <= 5 else 'weekends')        def AQI_LEVEL(x):            """            """            if 0.0 <= x <= 50.0:                return 'I'            elif 50.0 < x <= 100.0:                return 'II'            elif 100.0 < x <= 200.0:                return 'III'            elif 200.0 < x <= 300.0:                return 'IV'            elif x > 300.0:                return 'V'        airq_weather_canlendar['AQI_level'] = airq_weather_canlendar['AQI'].apply(AQI_LEVEL)        ## make goods info        category_his = data_sql['category_his'].drop(['date'], axis=1)        goods_his = data_sql['goods_his'].drop(['indate', 'date'], axis=1)        items_info = pd.merge(category_his, goods_his, on=['deptid'], how='right')        items_info = items_info.set_index('goodsid')        items_info_origin = items_info.copy()        le = LabelEncoder()        for col_name in items_info.columns:            if items_info[[col_name]].dtypes[0] == 'object':                items_info[col_name] = le.fit_transform(items_info[col_name].values)        for col_name in airq_weather_canlendar.columns:            if airq_weather_canlendar[[col_name]].dtypes[0] == 'object' and \                    col_name != 'storeid' and col_name != 'date':                airq_weather_canlendar[col_name] = le.fit_transform(airq_weather_canlendar[col_name].values)        condition = airq_weather_canlendar        print(u"condition 特征包含额的对象 ：{}".format(condition.columns))        condition['date'] = condition['date'].astype(str)        saleqty = data_sql['saleqty'].set_index(['date', 'storeid', 'goodsid'])        latest_price = data_sql['saleprice_his'][data_sql['saleprice_his'].date == self.FT(            self.TODAY_DATE, -1)].drop(['date'], axis=1)        price_fcst = pd.merge(data_sql['actual_purchase_fcst'].drop('cost', axis=1), latest_price, how='left',                              on=['goodsid', 'storeid'])        sale_price = pd.concat([data_sql['saleprice_his'], price_fcst]).set_index(            ['date', 'storeid', 'goodsid'])        actual_purchase_his = data_sql['actual_purchase_his'].set_index(['date', 'storeid', 'goodsid'])        purchase = pd.concat([data_sql['actual_purchase_his'], data_sql['actual_purchase_fcst']]).set_index(            ['date', 'storeid', 'goodsid'])        df_train_val = pd.concat([saleqty, sale_price, actual_purchase_his], axis=1).fillna(0.0).reset_index()        dfx_tag = pd.DataFrame(df_train_val[['goodsid', 'storeid',                                             'sale_qty']].groupby(            ['storeid', 'goodsid']).sale_qty.mean()).reset_index()        dfx_tag['X'] = dfx_tag['sale_qty'].map(lambda x: 1 if x < 100 else 1000)        dfx_tag = dfx_tag.drop(['sale_qty'], axis=1)        df_train_val['sale_qty'] = df_train_val['sale_qty'].map(            lambda x: x if x < 100 else x / 1000)        df_train_val_con = pd.merge(df_train_val, condition, on=['date', 'storeid'], how='left')        df_train_pre = pd.concat([saleqty, sale_price, purchase], axis=1).fillna(0.0).reset_index()        df_train_pre['sale_qty'] = df_train_pre['sale_qty'].map(            lambda x: x if x < 100 else x / 1000)        gy_price_distr = df_train_val_con.groupby(['goodsid', 'day_type']).apply(            lambda x: x['price'].abs().describe())        gy_price_distr.columns = ['price_' + col_nm for col_nm in gy_price_distr.columns]        gy_price_distr = gy_price_distr.reset_index().fillna(0.0)        gy_price_distr['price_stastic_meaning'] = gy_price_distr['price_count'].map(            lambda x: True if x > 35 else False)        gy_discount_distr = df_train_val_con.groupby(['goodsid', 'day_type']).apply(            lambda x: x['discount_degree'].abs().describe())        gy_discount_distr.columns = ['discount_' + col_nm for col_nm in gy_discount_distr.columns]        gy_discount_distr = gy_discount_distr.reset_index().fillna(0.0)        gy_discount_distr['discount_stastic_meaning'] = gy_discount_distr['discount_count'].map(            lambda x: True if x > 35 else False)        gy_saleqty_distr = df_train_val_con.groupby(['goodsid', 'day_type']).apply(            lambda x: x['sale_qty'].abs().describe())        gy_saleqty_distr.columns = ['saleqty_' + col_nm for col_nm in gy_saleqty_distr.columns]        gy_saleqty_distr = gy_saleqty_distr.reset_index().fillna(0.0)        gy_saleqty_distr['saleqty_stastic_meaning'] = gy_saleqty_distr['saleqty_count'].map(            lambda x: True if x > 35 else False)        gy_cost_distr = df_train_val_con.groupby(['goodsid', 'day_type']).apply(            lambda x: x['cost'].abs().describe())        gy_cost_distr.columns = ['cost_' + col_nm for col_nm in gy_cost_distr.columns]        gy_cost_distr = gy_cost_distr.reset_index().fillna(0.0)        gy_cost_distr['cost_stastic_meaning'] = gy_cost_distr['cost_count'].map(            lambda x: True if x > 35 else False)        gy_purchaseqty_distr = df_train_val_con.groupby(['goodsid', 'day_type']).apply(            lambda x: x['purchase_qty'].abs().describe())        gy_purchaseqty_distr.columns = ['purchaseqty_' + col_nm for col_nm in gy_purchaseqty_distr.columns]        gy_purchaseqty_distr = gy_purchaseqty_distr.reset_index().fillna(0.0)        gy_purchaseqty_distr['purchaseqty_stastic_meaning'] = gy_purchaseqty_distr['purchaseqty_count'].map(            lambda x: True if x > 35 else False)        print(u"验证集和训练集长度：", len(df_train_val))        print(u"训练集和预测集长度：", len(df_train_pre))        print(u"检查数据结构是否一致：", len(df_train_pre) - len(df_train_val) - len(data_sql['actual_purchase_fcst']))        features_info = {            'condition': condition,            'items_info': items_info,            'items_info_origin': items_info_origin,            'gy_price_distr': gy_price_distr,            'gy_discount_distr': gy_discount_distr,            'gy_saleqty_distr': gy_saleqty_distr,            'gy_cost_distr': gy_cost_distr,            'gy_purchaseqty_distr': gy_purchaseqty_distr,            'dfx_tag': dfx_tag        }        df_train_val['storage_0'] = df_train_val['purchase_qty'] - df_train_val['sale_qty']        df_train_pre['storage_0'] = df_train_pre['purchase_qty'] - df_train_pre['sale_qty']        df_train_val['storage_1'] = df_train_val['storage_0'] + df_train_val['purchase_qty'] - df_train_val[            'sale_qty']        df_train_pre['storage_1'] = df_train_pre['storage_0'] + df_train_pre['purchase_qty'] - df_train_pre[            'sale_qty']        df_train_val['storage_2'] = df_train_val['storage_1'] + df_train_val['purchase_qty'] - df_train_val[            'sale_qty']        df_train_pre['storage_2'] = df_train_pre['storage_1'] + df_train_pre['purchase_qty'] - df_train_pre[            'sale_qty']        print(u"""库存的特征 ok""")        gy_storage0_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_0'].describe())        gy_storage0_distr.columns = ['storage0_' + col_nm for col_nm in gy_storage0_distr.columns]        gy_storage0_distr = gy_storage0_distr.reset_index().fillna(0.0)        gy_storage0_distr['storage0_stastic_meaning'] = gy_storage0_distr['storage0_count'].map(            lambda x: True if x > 35 else False)        gy_storage1_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_1'].describe())        gy_storage1_distr.columns = ['storage1_' + col_nm for col_nm in gy_storage1_distr.columns]        gy_storage1_distr = gy_storage1_distr.reset_index().fillna(0.0)        gy_storage1_distr['storage1_stastic_meaning'] = gy_storage1_distr['storage1_count'].map(            lambda x: True if x > 35 else False)        gy_storage2_distr = df_train_val.groupby('goodsid').apply(lambda x: x['storage_2'].describe())        gy_storage2_distr.columns = ['storage2_' + col_nm for col_nm in gy_storage2_distr.columns]        gy_storage2_distr = gy_storage2_distr.reset_index().fillna(0.0)        gy_storage2_distr['storage2_stastic_meaning'] = gy_storage2_distr['storage2_count'].map(            lambda x: True if x > 35 else False)        df_train_pre = df_train_pre.drop(['storage_0', 'storage_1', 'storage_2', 'purchase_qty'], axis=1)        storage_info = {'gy_storage0_distr': gy_storage0_distr,                        'gy_storage1_distr': gy_storage1_distr,                        'gy_storage2_distr': gy_storage2_distr                        }        return features_info, storage_info, df_train_pre, df_train_valclass predict():    def __init__(self, sudoer, training_data, timeseries_feature):        self.training_data = training_data        self.TODAY_DATE = sudoer.TODAY_DATE        self.formatdate_a = sudoer.formatdate_a        self.before_job_days = sudoer.before_job_days        self.before_weekends_days = sudoer.before_weekends_days        self.PRE_DAYS = sudoer.PRE_DAYS        self.FT = sudoer.FT        self.LTD_weekends = sudoer.LTD_weekends        self.LTD_job = sudoer.LTD_job        self.MODEL = sudoer.MODEL        self.NAME_APP = sudoer.NAME_APP        self.VAL_date = sudoer.VAL_date        self.timeseries_feature = timeseries_feature        col_weekends = ['before_sale_qty_' + str(i).zfill(2) for i in range(0, self.before_weekends_days)]        col_jobs = ['before_sale_qty_' + str(i).zfill(2) for i in range(0, self.before_job_days)]        features_end_job = training_data['features_job'].reset_index().drop('index', axis=1)        features_end_weekends = training_data['features_weekends'].reset_index().drop('index', axis=1)        train_job = features_end_job[features_end_job['date'] < self.TODAY_DATE].set_index(            ['date', 'storeid', 'goodsid'])        train_weekends = features_end_weekends[features_end_weekends['date'] < self.TODAY_DATE].set_index(            ['date', 'storeid', 'goodsid'])        self.train_job = pd.concat(            [train_job, self.timeseries_feature['X_train'].set_index(['date', 'storeid', 'goodsid'])],            axis=0).reset_index().set_index(['date', 'storeid', 'goodsid', 'sale_qty'] + col_jobs).reset_index()        self.train_weekends = pd.concat(            [train_weekends, self.timeseries_feature['X_train'].set_index(['date', 'storeid', 'goodsid'])             ],            axis=0).reset_index().set_index(['date', 'storeid', 'goodsid', 'sale_qty'] + col_weekends).reset_index()        test_job = features_end_job[            features_end_job['date'] >= self.TODAY_DATE].set_index(['date', 'storeid', 'goodsid'])        test_weekends = features_end_weekends[            features_end_weekends['date'] >= self.TODAY_DATE].set_index(['date', 'storeid', 'goodsid'])        self.test_job = pd.concat(            [test_job, self.timeseries_feature['X_test'].set_index(['date', 'storeid', 'goodsid'])],            axis=0).reset_index().set_index(['date', 'storeid', 'goodsid', 'sale_qty'] + col_jobs).reset_index()        self.test_weekends = pd.concat(            [test_weekends, self.timeseries_feature['X_test'].set_index(['date', 'storeid', 'goodsid'])],            axis=0).reset_index().set_index(['date', 'storeid', 'goodsid', 'sale_qty'] + col_weekends).reset_index()    def data_combination(self, flag, num, pre_record, data_hat, final, final_result):        """        :param flag:        :param num:        :param pre_record:        :param data_hat:        :param final:        :param final_result:        :return:        """        train_job = self.train_job        train_weekends = self.train_weekends        train_weekends.to_csv('train_weekends.csv')        train_job.to_csv('train_job.csv')        test_job = self.test_job        test_weekends = self.test_weekends        test_job.to_csv('test_job.csv')        if num == 0:            if flag == '-job':                predict_day = test_job[test_job['date'] == pre_record]                data_to_model = pd.concat([train_job, predict_day], join_axes=[train_job.columns]).fillna(0.0)                data_hat = {'-job': data_to_model, '-weekends': train_weekends}                return data_to_model, data_hat            elif flag == '-weekends':                predict_day = test_weekends[test_weekends['date'].astype(str) == pre_record]                data_to_model = pd.concat([train_weekends, predict_day], join_axes=[train_weekends.columns]                                          ).fillna(0.0)                data_hat = {'-job': train_job, '-weekends': data_to_model}                return data_to_model, data_hat        else:            final = pd.DataFrame(final)            print(final.head())            final.columns = ['date', 'storeid', 'goodsid', 'sale_qty', 'int_sale_qty']            final_result = pd.DataFrame(final_result)            final_result.columns = ['date', 'storeid', 'goodsid', 'sale_qty', 'int_sale_qty']            ## insert week            final_result['week'] = final_result['date'].map(                lambda x: int(DT.datetime(int(x[:4]), int(x[5:7]), int(x[8:])).strftime("%w")))            df_0 = final_result[final_result['week'] == 0]            df_1 = final_result[final_result['week'] == 6]            final_weekends = pd.concat([df_0, df_1], axis=0)            final_job = final_result[0 < final_result['week']]            final_job = final_job[final_job['week'] < 6]            pre_record_1 = self.formatdate_a(pre_record, -1)            print(pre_record, pre_record_1)            WhichWeek = str(                DT.datetime(int(pre_record_1[:4]), int(pre_record_1[5:7]),                            int(pre_record_1[8:])).strftime("%w"))            if WhichWeek == '0' or WhichWeek == '6':                flag_l = '-weekends'                data_fresh_a = data_hat[flag_l]                latest_day_a = data_fresh_a[data_fresh_a['date'].astype(str) == pre_record_1]                latest_day_b = final[['sale_qty']]                latest_day_c = latest_day_a.drop(['sale_qty'], axis=1)                latest_day_c.reset_index(inplace=True)                latest_day_c.drop('index', axis=1, inplace=True)                latest_day_c.insert(3, 'sale_qty', latest_day_b)                df1 = data_fresh_a[data_fresh_a['date'].astype(str) < pre_record_1]                df2 = latest_day_c                data_fresh_b = pd.concat([df1, df2], join_axes=[df1.columns]                                         ).reset_index().drop('index', axis=1)                data_hat = {'-job': data_hat['-job'], '-weekends': data_fresh_b}            else:                flag_l = '-job'                data_fresh_a = data_hat[flag_l]                latest_day_a = data_fresh_a[data_fresh_a['date'].astype(str) == pre_record_1]                latest_day_b = final[['sale_qty']]                latest_day_c = latest_day_a.drop(['sale_qty'], axis=1)                latest_day_c.reset_index(inplace=True)                latest_day_c.drop('index', axis=1, inplace=True)                latest_day_c.insert(3, 'sale_qty', latest_day_b)                print(data_fresh_a.head(5))                df1 = data_fresh_a[data_fresh_a['date'].astype(str) < pre_record_1]                df2 = latest_day_c                data_fresh_b = pd.concat([df1, df2], join_axes=[df1.columns]                                         ).reset_index().drop('index', axis=1)                data_hat = {'-job': data_fresh_b, '-weekends': data_hat['-weekends']}            if flag == '-job':                predict_day = test_job[test_job['date'].astype(str) == pre_record]                num_job = len(set(final_job.date))                print('numbers of job days:', num_job)                col_job = ['before_sale_qty_' + str(i).zfill(2) for i in range(0, self.before_job_days)]                train_data = data_hat[flag]                latest_date = sorted(set(train_data.date.astype(str)))[-1]                la_train_data = train_data[train_data['date'].astype(str) == latest_date]                la_train_data_a = la_train_data[la_train_data.columns[3:3 + self.before_job_days]]                la_train_data_a.columns = col_job[0:self.before_job_days]                la_train_data_b = la_train_data[la_train_data.columns[1:3]]                la_train_data_c = pd.concat([la_train_data_b, la_train_data_a], axis=1)                predict_day = predict_day.drop(col_job[0:self.before_job_days], axis=1)                predict_day = pd.merge(predict_day, la_train_data_c, how='left', on=['storeid', 'goodsid']                                       )                data_to_model = pd.concat([df1, predict_day], join_axes=[df1.columns]                                          ).fillna(0.0).reset_index().drop('index', axis=1)                data_hat = {'-job': data_to_model, '-weekends': data_hat['-weekends']}                return data_to_model, data_hat            elif flag == '-weekends':                num_weekends = len(set(final_weekends.date))                print('numbers_weekends:', num_weekends)                predict_day = test_weekends[test_weekends['date'].astype(str) == pre_record]                ##T final                col = ['before_sale_qty_' + str(i).zfill(2) for i in range(0, self.before_weekends_days)]                train_data = data_hat[flag]                latest_date = sorted(set(train_data.date.astype(str)))[-1]                la_train_data = train_data[train_data['date'].astype(str) == latest_date]                la_train_data_a = la_train_data[la_train_data.columns[3:3 + self.before_weekends_days]]                la_train_data_a.columns = col[0:self.before_weekends_days]                la_train_data_b = la_train_data[la_train_data.columns[1:3]]                la_train_data_c = pd.concat([la_train_data_b, la_train_data_a], axis=1)                predict_day = predict_day.drop(col[0:self.before_weekends_days], axis=1)                predict_day = pd.merge(predict_day, la_train_data_c, how='left', on=['storeid', 'goodsid']                                       )                df1 = data_hat[flag]                data_to_model = pd.concat([df1, predict_day], ignore_index=True, join_axes=[df1.columns]                                          ).fillna(0.0).reset_index().drop('index', axis=1)                data_hat = {'-job': data_hat['-job'], '-weekends': data_to_model}                return data_to_model, data_hat    def exc_parall(self):        final_result = []        num = 0        for i in range(0, self.PRE_DAYS):            print(u'PREDICT FOR THE PREDICTED DAY:', self.FT(self.TODAY_DATE, i))            pre_record = self.FT(self.TODAY_DATE, i)            WhichWeek = str(                DT.datetime(int(pre_record[:4]), int(pre_record[5:7]),                            int(pre_record[8:])).strftime(                    "%w"))            if WhichWeek == '0' or WhichWeek == '6':                flag = '-weekends'                LTD_limit = self.LTD_weekends                print(u'*' * 80 + ' weekends_days ' + '*' * 80)            else:                flag = '-job'                LTD_limit = self.LTD_job                print(u'*' * 80 + ' work_days ' + '*' * 80)            if num == 0:                print(u'#' * 80 + u' 预测第一天 ' + '#' * 80)                data, data_hat = self.data_combination(flag, num, pre_record, 0, 0, 0)                dataset = self.train_valpre_set(data, pre_record, LTD_limit)                model_result = self.model(dataset, pre_record)                final = self.getting_result(model_result, pre_record, dataset)                final_result += final            else:                data, data_hat = self.data_combination(flag, num, pre_record, data_hat, final, final_result)                dataset = self.train_valpre_set(data, pre_record, LTD_limit)                model_result = self.model(dataset, pre_record)                final = self.getting_result(model_result, pre_record, dataset)                final_result += final            num = 1        return final_result    def train_valpre_set(self, data, pre_record, LTD_limit):        """        :param data:        :param pre_record:        :param LTD_limit:        :return:        """        print(u'#predict date:', pre_record)        data = data.fillna(0.0)        data = data.replace('NULL', 0.0)        feature_columns = data.columns[4:]        feature_map = {}        for i in range(0, len(feature_columns)):            feature_map['f' + str(i)] = feature_columns[i]        data = data[data.date.astype(str) >= self.FT(self.TODAY_DATE, -LTD_limit)]        data_train = data[data.date.astype(str) < self.FT(pre_record, -1)]        training_feature = data_train.iloc[:, 4:].astype('float64')        training_label = np.array(data_train.iloc[:, 3].astype('float64')).tolist()        data_val = data[data.date >= self.VAL_date]        data_val = data_val[data_val.date < self.TODAY_DATE]        val_feature = data_val.iloc[:, 4:].astype('float64')        val_label = np.array(data_val.iloc[:, 3].astype('float64')).tolist()        data_pre = data[data.date.astype(str) == pre_record]        predict_feature = data_pre.iloc[:, 4:].astype('float64')        predict_label = np.array(data_pre.iloc[:, 3].astype('float64')).tolist()        predict_goodsid = np.array(data_pre.iloc[:, 2]).tolist()        predict_store = np.array(data_pre.iloc[:, 1]).tolist()        predict_date = pre_record        print(u'##:', predict_date)        dataset = {"training_feature": training_feature,                   "training_label": training_label,                   "predict_goodsid": predict_goodsid,                   "predict_store": predict_store,                   "predict_feature": predict_feature,                   "predict_label": predict_label,                   "val_feature": val_feature,                   "val_label": val_label,                   "feature_map": feature_map                   }        return dataset    def model(self, dataset, pre_record):        """        :param dataset:        :param pre_record:        :return:        """        training_feature = dataset["training_feature"]        training_label = dataset["training_label"]        val_feature = dataset['val_feature']        val_label = dataset['val_label']        predict_feature = dataset["predict_feature"]        predict_label = dataset['predict_label']        feature_map = dataset["feature_map"]        if self.MODEL == 'GBDT':            model = GradientBoostingRegressor(n_estimators=650, learning_rate=0.01, max_depth=8, random_state=0,                                              loss='lad').fit(training_feature, training_label)            ## model predict result            predict_result = model.predict(predict_feature)            ## save the MODEL            joblib.dump(model, self.NAME_APP + pre_record + '#' + ".m")            return predict_result        elif self.MODEL == 'RFR':            params = {}            params['n_estimators'] = 200            params['n_jobs'] = 50            clf = RandomForestRegressor(n_estimators=200, n_jobs=50)            clf.fit(training_feature, training_label)            joblib.dump(clf, self.NAME_APP + pre_record + '#' + ".m")            predict_result = clf.predict(predict_feature)            return predict_result        elif self.MODEL == 'ETR':            clf = ExtraTreeRegressor(max_depth=8, random_state=1)            clf.fit(training_feature, training_label)            joblib.dump(clf, self.NAME_APP + pre_record + '#' + ".m")            predict_result = clf.predict(predict_feature)            return predict_result        elif self.MODEL == 'KNN':            clf = neighbors.KNeighborsRegressor()            clf.fit(training_feature, training_label)            joblib.dump(clf, self.NAME_APP + pre_record + '#' + ".m")            predict_result = clf.predict(predict_feature)            return predict_result        elif self.MODEL == 'XGBOOST':            print(u'XGboost 模型预测')            label = training_label            dtrain = xgb.DMatrix(training_feature, label)            num_round = 2000            params = {}            params["objective"] = "reg:linear"            params["eta"] = 0.05            params["max_depth"] = 17            params["eval_metric"] = "rmse"            params["silent"] = 1            params['nthread'] = 20            params['lambda'] = 1.0            params['gamma'] = 0.9            params['subsample'] = 0.9            params['colsample_bytree'] = 0.9            params['min_child_weight'] = 12            print(params)            plst = params  # Using 5000 rows for early stopping.            bst = xgb.train(plst, dtrain, num_round)            data_pre = xgb.DMatrix(predict_feature)            predict_result = bst.predict(data_pre)            joblib.dump(bst, self.NAME_APP + pre_record + '#' + ".m")            feature_scores = pd.Series(bst.get_fscore()).sort_values(ascending=False)            feature_scores = pd.DataFrame(feature_scores).reset_index()            feature_scores.columns = ['feature', 'feature_score']            feature_scores['feature_name'] = feature_scores['feature'].map(lambda x: feature_map[x])            feature_scores_normal = pd.Series(bst.get_fscore()).sort_values(ascending=False) / sum(                pd.Series(bst.get_fscore()).sort_values(ascending=False))            feature_scores.to_csv(self.NAME_APP + pre_record + '_feature_scores.csv')            feature_scores_normal.to_csv(self.NAME_APP + pre_record + '_feature_scores_normal.csv')            return predict_result        elif self.MODEL == "LightGBM":            params = {                'num_leaves': 256,                'objective': 'regression',                'min_data_in_leaf': 200,                'learning_rate': 0.002,                'feature_fraction': 0.8,                'bagging_fraction': 0.7,                'bagging_freq': 3,                'metric': 'l2',                'num_threads': 16            }            MAX_ROUNDS = 8000            cate_vars = []            dtrain = lgb.Dataset(                training_feature, label=training_label,                categorical_feature=cate_vars)            dval = lgb.Dataset(                val_feature, label=val_label, reference=dtrain,                categorical_feature=cate_vars)            bst = lgb.train(                params, dtrain, num_boost_round=MAX_ROUNDS,                valid_sets=[dtrain, dval], early_stopping_rounds=400, verbose_eval=50            )            print("\n".join(("%s: %.2f" % x) for x in sorted(                zip(training_feature.columns, bst.feature_importance("gain")),                key=lambda x: x[1], reverse=True            )))            val_pred = bst.predict(val_feature, num_iteration=bst.best_iteration or MAX_ROUNDS)            test_pred = bst.predict(predict_feature, num_iteration=bst.best_iteration or MAX_ROUNDS)            print("Validation mse:", mean_squared_error(val_label, val_pred))            err = (val_label - val_pred) ** 2            err = np.sqrt(err)            print('nwrmsle = {}'.format(err))            predict_result = test_pred            print("&&&&&&&")            print(test_pred)            return predict_result    def getting_result(self, predict_result, predict_date, dataset):        """        :param predict_result:        :param predict_date:        :param dataset:        :return:        """        predict_goodsid = dataset["predict_goodsid"]        predict_storeid = dataset["predict_store"]        csv_result = []        inside = []        for index in range(0, len(predict_result)):            inside.append(str(predict_date))            inside.append(str(predict_storeid[index]))            inside.append(str(predict_goodsid[index]))            inside.append((predict_result[index]))            forecast = round(predict_result[index])            inside.append(forecast)            csv_result.append(inside)            inside = []        return csv_resultclass Submission_result():    def __init__(self, sudoer, items_info_origin, dfx_tag, submission):        self.submission = submission        self.PRE_DAYS = sudoer.PRE_DAYS        self.items_info_origin = items_info_origin        self.dfx_tag = dfx_tag        self.NAME_APP = sudoer.NAME_APP    def submission_result(self):        dfx = self.dfx_tag        submission = pd.DataFrame(self.submission)        submission.columns = ['date', 'storeid', 'goodsid', 'sale_qty', 'sale_int_qty']        submission['goodsid'] = submission['goodsid'].astype(str)        submission['storeid'] = submission['storeid'].astype(str)        dfx['goodsid'] = dfx['goodsid'].astype(str)        dfx['storeid'] = dfx['storeid'].astype(str)        submission = pd.merge(submission, dfx, on=['storeid', 'goodsid'], how='left')        submission["pred_sale_volume"] = np.clip(np.expm1(submission["sale_qty"]), 0, 10000000)        submission["pred_sale_volume"] = submission["sale_qty"]        submission['pred_sale_volume'] = submission['pred_sale_volume'] * submission['X']        submission.insert(0, 'time_create', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))        params = {}        params['Model_name'] = self.NAME_APP.split('#')[0]        params['Prediction_days'] = self.PRE_DAYS        params['MAX_ROUND'] = 2000        json_str = json.dumps(params)        submission.insert(1, 'model_param', params['Model_name'])        submission['goodsid'] = submission['goodsid'].astype(str)        submission_item = pd.merge(            self.items_info_origin.reset_index()[['goodsid', 'name', 'spname']].astype(str),            submission, on=['goodsid'], how='right')        submission_item['goodsname'] = submission_item['name'] + [':'] + submission_item['spname']        submission_df = submission_item[['time_create', 'date',                                         'storeid', 'goodsid',                                         'goodsname', 'pred_sale_volume',                                         'model_param']]        submission_df.to_csv(self.NAME_APP + 'submission_data.csv')        print(submission_df.head(10))        self.sub_to_mysql(submission_df)    def sub_to_mysql(self, submission_df):        """        :param submission_df:        :return:        """        database_name = 'CHAOSHIFA_DB'        table_name = 'model_predct_result_dev'        columns_nm = ['time_create', 'date', 'storeid', 'goodsid',                      'goodsname', 'pred_sale_volume', 'model_param']        submission_df.columns = columns_nm        self.DataFrame_to_MySQL(submission_df, database_name, table_name)    def DataFrame_to_MySQL(self, dataframe, database_name, table_name):        """        :param dataframe:        :param database_name:        :param table_name:        :return:        """        yconnect = create_engine('mysql+pymysql://sophon:IAmSophon_4523@47.97.17.51:3306/?charset=utf8')        pd.io.sql.to_sql(dataframe, table_name, yconnect,                         schema=database_name, if_exists='append', index=False)def main():    """    :return:    """    DATE = parse_arguments(sys.argv[1:]).DATE    DATE = '2018-04-08'    print(DATE)    sudoer = Inition_Values(DATE)    ds = Get_mysql_data(sudoer)    dinfo = Get_mysql_data(sudoer, 'COMMONINFO_DB')    data_sql = {        'actual_purchase_his': ds.get_df_actual_purchase_his(),        'actual_purchase_fcst': ds.get_df_actualpurchase_fcst(),        'airq_his': ds.get_df_aggrbydate_airq_his(),        'airq_fcst': ds.get_df_airq_fcst(),        'saleprice_his': ds.get_df_aggrbydate_saleprice(),        'weather_his': ds.get_df_aggrbydate_weather_his(),        'weather_fcst': ds.get_df_weather_fcst(),        'saleqty': ds.get_df_aggrbydate_saleqty(),        'category_his': ds.get_df_category_his(),        'goods_his': ds.get_df_goods_his(),        'canlendar_info': dinfo.get_df_canlendar_info(),        'market_campaign_his': ds.get_market_campaign(),        'new_market_campaign_his': ds.df_get_campaign_his()    }    Pretor = Pre_data(sudoer, data_sql)    features_info, storage_info, df_train_pre, df_train_val = Pretor.pre_features()    promotion = Pretor.pre_promotion()    train_data = Get_Training_Data(sudoer, promotion, features_info, storage_info, df_train_pre)    features_done = train_data.pre_features_for_separate_data()    timeseries_featurer = Make_timeseries_features(sudoer, df_train_pre, features_info['items_info'])    timeseries_feature = timeseries_featurer.chart_data()    predictor = predict(sudoer, features_done, timeseries_feature)    submission_list = predictor.exc_parall()    Submissioner = Submission_result(sudoer, features_info['items_info_origin'], features_info['dfx_tag'],                                     submission_list)    Submissioner.submission_result()if __name__ == '__main__':    main()